{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1edacb55-2f6e-4da5-a63d-fccccb1b64bf",
   "metadata": {},
   "source": [
    "# word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab639b8-1b76-460c-acf0-e068c5463817",
   "metadata": {},
   "source": [
    "Consider a sentence:\n",
    "\n",
    "`the general [MASK] the troops`\n",
    "\n",
    "We want to predict the `[MASK]` token using the context words: `the`, `general`, `the`, `troops`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46d6a688-3b0d-412a-b88d-de2a78bef593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a71e845-4029-44a0-ad85-e8151eed4284",
   "metadata": {},
   "source": [
    "## CBOW\n",
    "\n",
    "The CBOW model for word2vec stands for Continuous Bag of Words.\n",
    "\n",
    "We take the average (mean) of the word vectors for the context words and use it to predict the target word (the word that can be viewed as the `[MASK]` token). \n",
    "\n",
    "This is an example of self-supervised learning because each `[MASK]` token is created from some text and so we know the right output word in each training example.\n",
    "\n",
    "The original model used two different embeddings: one for the target words as they appear as `[MASK]` tokens and another for the context words used to predict the target word. \n",
    "\n",
    "Pytorch has a simple way to go from one-hot vectors based on word indices to word embeddings (aka word vectors) using the `nn.Embedding` class.\n",
    "\n",
    "We create a hidden layer in this simple neural network by taking the mean of the context words. Pytorch forces you to think in terms of a batch of training/test examples at a time and it is typically the first dimension of the tensor we create. \n",
    "\n",
    "To take the mean of the input embeddings we start with the embeddings for all the inputs using a tensor of size `B x N x E` where `B` is the batch size, `N` is the number of inputs and `E` is the embedding size. We take the mean of dimension 1 (the `N` dimension) and we get a new tensor of size `B x 1 x E` which we can convert to a `B x E` tensor using the Pytorch `squeeze` function.\n",
    "\n",
    "We pass this through a simple linear network (fully connected single hidden layer) to get the hidden representation which is then used to predict the target word by mapping it back up to the vocabulary size using `self.expand` in this implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2759ee32-6b72-4505-963d-53c1d2a7025c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, embedding_size=100, vocab_size=-1):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.expand = nn.Linear(embedding_size, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        hidden = self.embed(inputs).mean(1).squeeze(1) # batch size x embedding_size\n",
    "        return self.expand(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6a03df-9f90-4a43-a1b7-7ccdfd7a8291",
   "metadata": {},
   "source": [
    "### Create the dataset\n",
    "\n",
    "The dataset creation reads the raw text data and creates the word to index and index to word dictionaries.\n",
    "\n",
    "For a window size of 2 it creates each instance for training which has a `context` of a list of context words and the target `mask_token` which is the answer the neural network should predict to get zero loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e422d6d-ce11-4fd8-b2e4-503006c13a4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Fulton',\n",
       " 'County',\n",
       " 'Grand',\n",
       " 'Jury',\n",
       " 'said',\n",
       " 'Friday',\n",
       " 'an',\n",
       " 'investigation',\n",
       " 'of']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "brown.words()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "369ca853-2340-40c2-9d3b-3d9049e3456e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Instance = namedtuple('Instance', ['context', 'mask_token'])\n",
    "\n",
    "def create_dataset():\n",
    "    tokenized_text = [w.lower() for w in brown.words()]\n",
    "    vocab = set(tokenized_text)\n",
    "    word_to_idx = dict()\n",
    "    idx_to_word = dict()\n",
    "\n",
    "    # create word to index mapping and vice versa\n",
    "    for i, word in enumerate(vocab):\n",
    "        word_to_idx[word] = i\n",
    "        idx_to_word[i] = word\n",
    "\n",
    "    # generate training data using two words of context before and after [MASK] \n",
    "    data = []\n",
    "    for i in range(2, len(tokenized_text)-2):\n",
    "        context = [\n",
    "            tokenized_text[i-2],\n",
    "            tokenized_text[i-1],\n",
    "            tokenized_text[i+1],\n",
    "            tokenized_text[i+2],\n",
    "        ]\n",
    "        masked_token = tokenized_text[i]\n",
    "        context_idxs = [word_to_idx[w] for w in context]\n",
    "        mask_token_idx = word_to_idx[masked_token]\n",
    "        # data is a list of context, mask_token tuples\n",
    "        data.append(Instance(context=context_idxs, mask_token=mask_token_idx))\n",
    "    \n",
    "    return data, word_to_idx, idx_to_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5b33bb-97f0-44c6-af0b-009e0034f669",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "Training the model is a fairly standard loop in Pytorch which doesn't change much once the model is defined and it knows how to compute the forward pass through the model. The backward pass through the model for standard `nn` components in Pytorch is done automatically so you don't need to specify the `backward` function yourself. If you create a novel `nn.Module` component that does not use standard Pytorch modules then you will need to specify the `backward` function for that module.\n",
    "\n",
    "We are using the Pytorch `DataLoader` class to create batches of training for the core training loop.\n",
    "\n",
    "Typically introductory training loops use a number of epochs (each epoch is one pass over the training data). Instead, below the loop uses a fixed number of updates to the model and reports the loss after `show_loss` number of updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63fc9378-c977-4b2a-9c47-afac38eb15b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_updates = 10000\n",
    "show_loss = 100\n",
    "\n",
    "def train():\n",
    "    data, word_to_idx, idx_to_word = create_dataset()\n",
    "    print(\"finished reading dataset\")\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    model = CBOW(embedding_size=100, vocab_size=len(word_to_idx))\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    context_data = torch.tensor([instance.context for instance in data])\n",
    "    output = torch.tensor([instance.mask_token for instance in data])\n",
    "\n",
    "    # create dataset using the pytorch dataloader\n",
    "    dataset = torch.utils.data.TensorDataset(context_data, output)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    step = 0\n",
    "    while step < num_updates:\n",
    "        for context, true_output in dataloader:\n",
    "            step += 1\n",
    "            output = model(context)\n",
    "            loss = loss_func(output, true_output)\n",
    "            if (step % show_loss == 0):\n",
    "                print(f\"Step: {step}, Loss: {loss.item()}\")\n",
    "            if (step > num_updates):\n",
    "                break\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    return model, word_to_idx, idx_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668be1a0-ce54-43d6-b94e-fc6cf12a7497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished reading dataset\n",
      "Step: 100, Loss: 10.80092716217041\n",
      "Step: 200, Loss: 10.750906944274902\n",
      "Step: 300, Loss: 10.628578186035156\n",
      "Step: 400, Loss: 10.664700508117676\n",
      "Step: 500, Loss: 10.61719036102295\n",
      "Step: 600, Loss: 10.50640869140625\n",
      "Step: 700, Loss: 10.49307632446289\n",
      "Step: 800, Loss: 10.433218002319336\n",
      "Step: 900, Loss: 10.255248069763184\n",
      "Step: 1000, Loss: 10.208154678344727\n",
      "Step: 1100, Loss: 10.220098495483398\n",
      "Step: 1200, Loss: 10.235394477844238\n",
      "Step: 1300, Loss: 10.047728538513184\n",
      "Step: 1400, Loss: 9.820001602172852\n",
      "Step: 1500, Loss: 9.678244590759277\n",
      "Step: 1600, Loss: 10.128551483154297\n",
      "Step: 1700, Loss: 9.592265129089355\n",
      "Step: 1800, Loss: 9.806705474853516\n",
      "Step: 1900, Loss: 9.34371280670166\n",
      "Step: 2000, Loss: 9.595269203186035\n",
      "Step: 2100, Loss: 9.311166763305664\n",
      "Step: 2200, Loss: 9.567536354064941\n",
      "Step: 2300, Loss: 9.2586088180542\n",
      "Step: 2400, Loss: 9.253110885620117\n",
      "Step: 2500, Loss: 9.012722969055176\n",
      "Step: 2600, Loss: 9.125011444091797\n",
      "Step: 2700, Loss: 9.180374145507812\n",
      "Step: 2800, Loss: 9.246970176696777\n",
      "Step: 2900, Loss: 9.136131286621094\n",
      "Step: 3000, Loss: 9.125503540039062\n"
     ]
    }
   ],
   "source": [
    "model, word_to_idx, idx_to_word = train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aefa41d-7c76-49e3-961e-16fc04a0db5b",
   "metadata": {},
   "source": [
    "### Using the model\n",
    "\n",
    "The model once trained can be used for the word embeddings it has learned. In the following function `get_k_closest_words` we return the words with the closest cosine similarity to some word we are interested in learning about.\n",
    "\n",
    "Due to limitations of the size of training data and number of updates to the model being limited as well, the similar words list is not as compelling as with previously pre-trained word embeddings like word2vec or GLoVe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3c27253-25ff-4053-9892-1043500a7df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def get_k_closest_words(word, k=10):\n",
    "    embeddings = model.embed.weight.data\n",
    "    word_idx = torch.tensor(word_to_idx[word], dtype=torch.int)\n",
    "    word_embedding = embeddings[word_idx]\n",
    "    similarities = F.cosine_similarity(embeddings, word_embedding)\n",
    "    sorted_indices = torch.argsort(similarities, descending=True)\n",
    "    top_k_idx = [i.item() for i in sorted_indices[1:k+1]]\n",
    "    top_k = [idx_to_word[i] for i in top_k_idx]\n",
    "    similarity_scores = [similarities[i].item() for i in top_k_idx]\n",
    "    return top_k, similarity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe55997e-ef68-424a-8a17-8666abbd03bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['spigots',\n",
       "  'nihilistic',\n",
       "  'cereals',\n",
       "  'articulation',\n",
       "  'lath',\n",
       "  'alloy',\n",
       "  'justifying',\n",
       "  'hydrogens',\n",
       "  'lust',\n",
       "  'triangular'],\n",
       " [0.48290154337882996,\n",
       "  0.3924635648727417,\n",
       "  0.3625154197216034,\n",
       "  0.3580115735530853,\n",
       "  0.3536774218082428,\n",
       "  0.3453938961029053,\n",
       "  0.34470126032829285,\n",
       "  0.3435172736644745,\n",
       "  0.34129834175109863,\n",
       "  0.33940404653549194])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_k_closest_words('give')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e374bb7-04a9-4d85-81f3-611e15a9337d",
   "metadata": {},
   "source": [
    "## Using gensim for word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16d518e-98bd-44d8-b6b7-f48cd8d8778f",
   "metadata": {},
   "source": [
    "`gensim` uses SkipGram training for word2vec which can be trained on CPUs more effectively compared to the more neural network style training required for CBOW.\n",
    "\n",
    "SkipGram training for word2vec only requires a dataset of positive and negative word pairs:\n",
    "\n",
    "```\n",
    "(target_word, context_word) TRUE/FALSE\n",
    "```\n",
    "\n",
    "The `TRUE` cases occur in the data with the `context_word` appearing in the context window. The `FALSE` cases are constructed using \"negative sampling\" which just means that we sample from the space of context words looking for words which are likely to be distractors or negatively correlated with the `target_word`.\n",
    "\n",
    "The training data is constructed is this way so that all we need to do is train a binary classifier using this self supervised dataset. The end result is still the \"hidden\" embeddings learned while training this classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "637a4241-de39-468d-8409-4b85c018c6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "be1cc1fd-6e7e-4753-9cd8-15fa48f253d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produced `` no evidence '' that any irregularities took place .\n",
      "The jury further said in term-end presentments that the City Executive Committee , which had over-all charge of the election , `` deserves the praise and thanks of the City of Atlanta '' for the manner in which the election was conducted .\n",
      "The September-October term jury had been charged by Fulton Superior Court Judge Durwood Pye to investigate reports of possible `` irregularities '' in the hard-fought primary which was won by Mayor-nominate Ivan Allen Jr. .\n",
      "`` Only a relative handful of such reports was received '' , the jury said , `` considering the widespread interest in the election , the number of voters and the size of this city '' .\n",
      "The jury said it did find that many of Georgia's registration and election laws `` are outmoded or inadequate and often ambiguous '' .\n",
      "It recommended that Fulton legislators act `` to have these laws studied and revised to the end of modernizing and improving them '' .\n",
      "The grand jury commented on a number of other topics , among them the Atlanta and Fulton County purchasing departments which it said `` are well operated and follow generally accepted practices which inure to the best interest of both governments '' .\n",
      "Merger proposed\n",
      "However , the jury said it believes `` these two offices should be combined to achieve greater efficiency and reduce the cost of administration '' .\n",
      "The City Purchasing Department , the jury said , `` is lacking in experienced clerical personnel as a result of city personnel policies '' .\n"
     ]
    }
   ],
   "source": [
    "sentences = brown.sents()\n",
    "print(\"\\n\".join([ \" \".join(s) for s in sentences[:10]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3fda8b07-6cff-431b-94fb-f5be7ff51412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done training w2v_win2\n"
     ]
    }
   ],
   "source": [
    "d = 100 # dimension of the word vectors\n",
    "w2v_win2 = Word2Vec(sentences, vector_size=d, window=2, min_count=5, negative=15, epochs=10, workers=multiprocessing.cpu_count())\n",
    "print(\"done training w2v_win2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a2d0d93a-f8e6-4537-bd84-0c3115ec8853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done training w2v_win5\n"
     ]
    }
   ],
   "source": [
    "w2v_win5 = Word2Vec(sentences, vector_size=d, window=5, min_count=5, negative=15, epochs=10, workers=multiprocessing.cpu_count())\n",
    "print(\"done training w2v_win5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b51d3145-ca33-4e46-9922-7e4b646b9743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Monday', 0.8832598328590393), ('Friday', 0.8800375461578369), ('Sunday', 0.8766680359840393)]\n",
      "[('job', 0.7046144604682922), ('care', 0.6790106296539307), ('advantage', 0.6556699275970459)]\n",
      "[('person', 0.7763979434967041), ('artist', 0.7265687584877014), ('woman', 0.721325159072876)]\n",
      "[('plus', 0.4737573266029358), ('common', 0.4588683545589447), ('literary', 0.4464430510997772)]\n"
     ]
    }
   ],
   "source": [
    "vecs = w2v_win5.wv\n",
    "print(vecs.similar_by_word(\"Saturday\")[:3])\n",
    "print(vecs.similar_by_word(\"money\")[:3])\n",
    "print(vecs.similar_by_word(\"child\")[:3])\n",
    "print(vecs.similar_by_word(\"of\")[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4da26b96-ebc1-4a15-ab31-6914f484a682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('13', 0.9102800488471985), ('18', 0.8949381113052368), ('19', 0.886096179485321), ('11', 0.8809764385223389), ('21', 0.87760329246521), ('14', 0.8719170093536377), ('Oct.', 0.8694112300872803), ('24', 0.8691827654838562), ('23', 0.8612421154975891), ('Nov.', 0.8607510924339294)]\n"
     ]
    }
   ],
   "source": [
    "print(vecs.most_similar(positive=['12', '8'], negative=['5']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ac9847f0-baa3-4723-a3a5-9fb14b992898",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gigaword = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f1deed-79ea-4ed6-9cc5-8f0b903c1eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gigaword.most_similar(\"man\", topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "43d6e2c8-a29d-49fa-975e-1dd2bc801d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.7698541283607483), ('monarch', 0.6843380331993103), ('throne', 0.6755736470222473), ('daughter', 0.6594556570053101), ('princess', 0.6520534157752991), ('prince', 0.6517034769058228), ('elizabeth', 0.6464518308639526), ('mother', 0.631171703338623), ('emperor', 0.6106470823287964), ('wife', 0.6098655462265015)]\n"
     ]
    }
   ],
   "source": [
    "print(model_gigaword.most_similar(positive=['king', 'woman'], negative=['man']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "9394c304-f1e1-4ae7-881f-e5b4274bb13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('16', 0.9776545763015747), ('14', 0.9762073755264282), ('13', 0.9723331332206726), ('17', 0.9680772423744202), ('19', 0.9613232016563416), ('22', 0.9609626531600952), ('15', 0.9585664868354797), ('21', 0.9549593925476074), ('11', 0.9540944695472717), ('23', 0.9537526965141296)]\n"
     ]
    }
   ],
   "source": [
    "print(model_gigaword.most_similar(positive=['12', '8'], negative=['5']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

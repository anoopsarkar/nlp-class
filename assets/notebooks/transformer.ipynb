{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf55feea-7c94-4c5d-8a0c-a0602005f35f",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c331b81-d655-4a30-ab52-2be5b5f78292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11aaed4b-1e8f-42cb-b054-6a4d15a4013f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "c0aded15-3378-4bf6-8644-3d7155b85e66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x114f16770>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5974049c-999b-4d2c-b1c3-b7db619f922d",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3197314-abad-4a20-baea-54e2d9621490",
   "metadata": {},
   "source": [
    "We are going to use the [wikipron](https://github.com/CUNY-CL/wikipron) dataset `v1.3.0` which contains a database of over 3M word/pronunciation pairs from 251 languages scraped from [Wiktionary](https://en.wiktionary.org/wiki/Wiktionary:Main_Page).\n",
    "\n",
    "The pronunciations are in the [International Phonetic Alphabet](https://en.wikipedia.org/wiki/International_Phonetic_Alphabet) (IPA) format.\n",
    "\n",
    "The dataset is described in detail in the following paper:\n",
    "\n",
    "> Jackson L. Lee, Lucas F.E. Ashby, M. Elizabeth Garza, Yeonju Lee-Sikka, Sean Miller, Alan Wong, Arya D. McCarthy, and Kyle Gorman (2020). [Massively multilingual pronunciation mining with WikiPron](https://www.aclweb.org/anthology/2020.lrec-1.521/). In Proceedings of the 12th Language Resources and Evaluation Conference, pages 4223-4228.\n",
    "\n",
    "The github repo contains more up to date scrape of data from Wiktionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4ee6729e-e978-4def-9ac4-d8b75beab24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "afr afr_latn_narrow.tsv\n",
      "ang ang_latn_narrow.tsv\n",
      "apw apw_latn_narrow.tsv\n",
      "ast ast_latn_narrow.tsv\n",
      "aze aze_latn_narrow.tsv\n",
      "aze aze_latn_narrow_filtered.tsv\n",
      "ces ces_latn_narrow.tsv\n",
      "cor cor_latn_narrow.tsv\n",
      "cym cym_latn_nw_narrow.tsv\n",
      "cym cym_latn_sw_narrow.tsv\n",
      "dan dan_latn_narrow.tsv\n",
      "deu deu_latn_narrow.tsv\n",
      "dsb dsb_latn_narrow.tsv\n",
      "eng eng_latn_uk_narrow.tsv\n",
      "eng eng_latn_us_narrow.tsv\n",
      "epo epo_latn_narrow.tsv\n",
      "eus eus_latn_narrow.tsv\n",
      "fao fao_latn_narrow.tsv\n",
      "fin fin_latn_narrow.tsv\n",
      "fra fra_latn_narrow.tsv\n",
      "gle gle_latn_narrow.tsv\n",
      "glg glg_latn_narrow.tsv\n",
      "glv glv_latn_narrow.tsv\n",
      "hau hau_latn_narrow.tsv\n",
      "haw haw_latn_narrow.tsv\n",
      "hun hun_latn_narrow.tsv\n",
      "hun hun_latn_narrow_filtered.tsv\n",
      "huu huu_latn_narrow.tsv\n",
      "iba iba_latn_narrow.tsv\n",
      "ilo ilo_latn_narrow.tsv\n",
      "ind ind_latn_narrow.tsv\n",
      "isl isl_latn_narrow.tsv\n",
      "lat lat_latn_clas_narrow.tsv\n",
      "lat lat_latn_eccl_narrow.tsv\n",
      "lav lav_latn_narrow.tsv\n",
      "lav lav_latn_narrow_filtered.tsv\n",
      "lim lim_latn_narrow.tsv\n",
      "lit lit_latn_narrow.tsv\n",
      "lmo lmo_latn_narrow.tsv\n",
      "lmy lmy_latn_narrow.tsv\n",
      "ltz ltz_latn_narrow.tsv\n",
      "mah mah_latn_narrow.tsv\n",
      "mak mak_latn_narrow.tsv\n",
      "mic mic_latn_narrow.tsv\n",
      "msa msa_latn_narrow.tsv\n",
      "nci nci_latn_narrow.tsv\n",
      "nhg nhg_latn_narrow.tsv\n",
      "nld nld_latn_narrow.tsv\n",
      "nmy nmy_latn_narrow.tsv\n",
      "nno nno_latn_narrow.tsv\n",
      "nob nob_latn_narrow.tsv\n",
      "oci oci_latn_narrow.tsv\n",
      "pjt pjt_latn_narrow.tsv\n",
      "por por_latn_bz_narrow.tsv\n",
      "por por_latn_po_narrow.tsv\n",
      "ron ron_latn_narrow.tsv\n",
      "ron ron_latn_narrow_filtered.tsv\n",
      "rup rup_latn_narrow.tsv\n",
      "scn scn_latn_narrow.tsv\n",
      "sco sco_latn_narrow.tsv\n",
      "sga sga_latn_narrow.tsv\n",
      "slk slk_latn_narrow.tsv\n",
      "spa spa_latn_ca_narrow.tsv\n",
      "spa spa_latn_la_narrow.tsv\n",
      "sqi sqi_latn_narrow.tsv\n",
      "swe swe_latn_narrow.tsv\n",
      "tby tby_latn_narrow.tsv\n",
      "tft tft_latn_narrow.tsv\n",
      "tgl tgl_latn_narrow.tsv\n",
      "tur tur_latn_narrow.tsv\n",
      "tur tur_latn_narrow_filtered.tsv\n",
      "vie vie_latn_hanoi_narrow.tsv\n",
      "vie vie_latn_hanoi_narrow_filtered.tsv\n",
      "vie vie_latn_hcmc_narrow.tsv\n",
      "vie vie_latn_hcmc_narrow_filtered.tsv\n",
      "vie vie_latn_hue_narrow.tsv\n",
      "vie vie_latn_hue_narrow_filtered.tsv\n",
      "vol vol_latn_narrow.tsv\n",
      "xho xho_latn_narrow.tsv\n",
      "ycl ycl_latn_narrow.tsv\n",
      "zza zza_latn_narrow.tsv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "from zipfile import ZipFile, is_zipfile\n",
    "from collections import defaultdict\n",
    "random.seed(42)\n",
    "\n",
    "datafile = 'wikipron-data-1.3.0.zip'\n",
    "constraints = ['narrow', 'latn']\n",
    "data = defaultdict(list)\n",
    "\n",
    "if is_zipfile(datafile):\n",
    "    with ZipFile(datafile) as wpzip:\n",
    "        for filename in wpzip.namelist():\n",
    "            features = filename.split('_')\n",
    "            lang = features[0]\n",
    "            matches = [ True if re.search(c, filename) else False for c in constraints ]\n",
    "            if all(matches):\n",
    "                print(lang, filename)\n",
    "                with wpzip.open(filename, 'r') as fh:\n",
    "                    data[lang] += [ s.decode('utf8').strip() for s in fh.readlines() ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1712e7e-09c0-4e14-9a9a-6c03e4330434",
   "metadata": {},
   "source": [
    "Let's check the format of the data for English. Each language is stored as a separate key in the `data` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "39e869d8-9695-4b17-8c93-70a4c2413af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'em\tm̩\n",
      "'em\tə m\n",
      "'em\tɛ m\n",
      "'em\tɪ m\n",
      "'ll\tl̩\n",
      "'ll\tə l\n",
      "'ll\tɫ̩\n",
      "'ll\tɯ\n",
      "'ll\tʊ\n",
      "'n'\tn̩\n",
      "'n'\tə n\n",
      "'rales\tɹ ɑː l i z\n",
      "aa\tɑː ʔ ɑː\n",
      "abear\tə b ɛː\n",
      "accend\tæ k s ɛ n d\n",
      "accommodate\tə kʰ ɒ m ə d e ɪ t\n",
      "accommodator\tə k ɒ m ə d e ɪ t ə\n",
      "accreditation\tə k ɹ ɛ d ɪ t e ɪ ʃ ə n\n",
      "acknowledge\tə k n ɒ l ɨ̞ d͡ʒ\n",
      "acknowledge\tə ɡ n ɒ l ɨ̞ d͡ʒ\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(data['eng'][:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ae9daba4-88fe-4c7f-b2c1-47960e367d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "start = '<|starttext|>'\n",
    "to_ipa = '<|to_ipa|>'\n",
    "endoftext = '<|endoftext|>'\n",
    "padtoken = '<|padtoken|>'\n",
    "languages = ['eng']\n",
    "#languages = data.keys()  # data.keys() for all languages\n",
    "\n",
    "for lang in languages:\n",
    "    for i in data[lang]:\n",
    "        (source, target) = i.split('\\t')\n",
    "        dataset.append([start] + list(source) + [to_ipa] + target.split(' ') + [endoftext] + [padtoken])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c4d00b47-d9e6-4849-a042-b840b092a3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<|starttext|>', \"'\", 'e', 'm', '<|to_ipa|>', 'm̩', '<|endoftext|>', '<|padtoken|>'], ['<|starttext|>', \"'\", 'e', 'm', '<|to_ipa|>', 'ə', 'm', '<|endoftext|>', '<|padtoken|>'], ['<|starttext|>', \"'\", 'e', 'm', '<|to_ipa|>', 'ɛ', 'm', '<|endoftext|>', '<|padtoken|>'], ['<|starttext|>', \"'\", 'e', 'm', '<|to_ipa|>', 'ɪ', 'm', '<|endoftext|>', '<|padtoken|>'], ['<|starttext|>', \"'\", 'l', 'l', '<|to_ipa|>', 'l̩', '<|endoftext|>', '<|padtoken|>']]\n"
     ]
    }
   ],
   "source": [
    "print(dataset[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d74d560-ddc9-4c4b-9e3a-e77af7e5399b",
   "metadata": {},
   "source": [
    "Let's collect the vocabulary from the dataset. This will be used to encode each example in the dataset to a tensor and when we get output from our model we can decode it back to the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "721a5854-a8ee-4359-bb61-8f26938e1242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'<|endoftext|><|padtoken|><|starttext|><|to_ipa|>aaʰaːaːːbbʰbˡb̚b̥b̪cddˀd̚d̠d̠͡ɹ̠d̥d̥̚d̪d͡ɹ̝d͡ʒdⁿeeːe̞e̯e͡əeᵊffʷf̚f͡s̪ghiiːiːʲiˑi̯jjːj̊j̥kkʰkʲkʷk̚k̠ʰk͡sllʷl̥l̥ʰl̩l̪mm̥m̩m̩ːnnʰnːn̠n̥n̩n̪n̪̥ooːo̞o̞ːppʰpʷpʼp̚p̪p͡tʼʰp͡ɸːqrsttʰtˀtːtˤt̚t̠t̠ʰt̠̚t̠͡ɹ̠t̠͡ɹ̠̊t̠͡ʃt̪t̪ʰt̬t̰̚t͡st͡ʃt͡ʃʰtᵊtⁿuuʷuːu̟u̟ːu̠ːu̥u̯vww̥xyyːzz̥~áãääːææˀæːæːːæˑæ̃æ̙æ̝æ̝ːæ̰ˀæ̰ːçéëðõö̞øúüăčĩĩːĭ̥ŋŋ̍ŏœːŭ̥ǀɐɐːɑɑˀɑːɑˑɑ̃ɑ̃ːɑ̃ⁿɑ̟ɒɒˀɒːɒ̃ːɒ̃ⁿɒ͡ɪɔɔːɔ̃ɕɖɘəəːə̃ə̆ə̥ə̯ə̯̃ɚɚːɚːːɚ̯ɛɛːɛˑɛ̃ɛ̈ɛ͡əɜɜːɝɝːɝˑɝ̃ɡɡ̊ɣɣːɤɥɦɨɨ̃ɨ̞ɪɪʰɪːɪˑɪ̃ɪ̃ːɪ̈ɪ̞ɪ̥ɪ̯ɫɫ̥ɫ̩ɯɯ᷈ːɱɱ̊ɱ̩ɲɵɸɸ͡sɸ͡s̪ɹɹʲɹʷɹˠɹ̝̊ɹ̠ɹ̠ʷɹ̠̊ɹ̥ɹ̥ʷɹ̩ɻɻʷɽ̃ɾɾʱɾ̃ɾ̪̊ɾᵊʀ̊ʁʃʉʉːʉ̃͡uʉ̜ːʉ̞ʉ̯ʉ͡uʊʊʷʊːʊ̃ʊ̆ʊ̈ʊ̠ʊ̯ʊ̯ˀʊ̯ːʊ̯̃ʋʌʌˀʌˑʌ̃ʌ̃ːʌ᷈ʍʎʏʒʔʔ̚ʔᵊʙ̥͡ɸːʝʟ̩ˀeː˔˔ʷ˗ʷ˞˥˥˩˦˨˨βθχẽẽːẽˑ\n",
      "vocab_size=312\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(list({ c for ex in dataset for c in ex }))\n",
    "vocab_size = len(vocab)\n",
    "print(''.join(vocab))\n",
    "print(f'{vocab_size=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229da522-40b3-4a3c-b521-03f774fae94d",
   "metadata": {},
   "source": [
    "First we create a mapping `stoi` from the vocabulary to integers, and vice versa `itos`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "caebfec4-3bcf-4bd7-b968-a1b60330147a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = { ch:i for i, ch in enumerate(vocab) }\n",
    "itos = { i:ch for i, ch in enumerate(vocab) }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1e9af7-b730-4aae-a2bb-b9c0a03fc87c",
   "metadata": {},
   "source": [
    "Then we use this mapping to create `encode` and `decode` functions from raw input to integers and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bca7fce6-caa1-4d6c-ac21-6c9846d6062f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = lambda input: [stoi[c] for c in input]\n",
    "decode = lambda ints: [itos[i] for i in ints]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9dc7ae44-8fd2-4337-a0eb-e822b652e670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 0, 27, 61, 4, 63, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "print(encode(dataset[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c1b14a09-56e5-42e1-b8f6-b0d4db40d90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|starttext|>', \"'\", 'e', 'm', '<|to_ipa|>', 'm̩', '<|endoftext|>']\n"
     ]
    }
   ],
   "source": [
    "print(decode(encode(dataset[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "fef17795-912c-4279-b75f-91718a2fd785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_longest(dataset):\n",
    "    max_len = 0\n",
    "    longest = None\n",
    "    idx = None\n",
    "    for i, ex in enumerate(dataset):\n",
    "        if len(ex) > max_len:\n",
    "            idx = i\n",
    "            max_len = len(ex)\n",
    "            longest = ex\n",
    "    return(idx, max_len, longest)\n",
    "\n",
    "(longest_idx, max_len, longest) = get_longest(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f6efd22c-61b1-41b7-ac12-31ffdef5baee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "380 43 ['<|starttext|>', 'd', 'e', 'a', 'c', 'e', 't', 'y', 'l', 't', 'r', 'a', 'n', 's', 'f', 'e', 'r', 'a', 's', 'e', '<|to_ipa|>', 'd', 'iː', 'ə', 's', 'iː', 't', 'a', 'ɪ', 'ɫ', 't', 'ɹ', 'ɑː', 'n', 's', 'f', 'ə', 'ɹ', 'e', 'ɪ', 'z', '<|endoftext|>', '<|padtoken|>']\n"
     ]
    }
   ],
   "source": [
    "print(longest_idx, max_len, longest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6621da9-fe64-41ac-926b-41231d74181f",
   "metadata": {},
   "source": [
    "We pad each example in the dataset to be the same as the longest example. This will ensure we can have same length tensors in each batch of training. At the same time we will convert the dataset using the `encode` function to a torch `tensor` object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4fc0148d-27ba-4022-9335-765a26e791db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([ 3,  0, 27, 61,  4, 63,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  2,  2,  2,  2]), tensor([  3,   0,  27,  61,   4, 183,  61,   1,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   0,  27,  61,   4, 194,  61,   1,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   0,  27,  61,   4, 216,  61,   1,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([ 3,  0, 55, 55,  4, 59,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  2,  2,  2,  2]), tensor([  3,   0,  55,  55,   4, 183,  55,   1,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   0,  55,  55,   4, 228,   1,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   0,  55,  55,   4, 229,   1,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   0,  55,  55,   4, 268,   1,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([ 3,  0, 65,  0,  4, 70,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  2,  2,  2,  2]), tensor([  3,   0,  65,   0,   4, 183,  65,   1,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   0,  86,   5,  55,  27,  87,   4, 239, 165,  55,  39, 123,   1,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,   5,   4, 165, 290, 165,   1,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,   9,  27,   5,  86,   4, 183,   9, 195,   1,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  15,  15,  27,  65,  16,   4, 130,  48,  87, 194,  65,  16,\n",
      "          1,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  15,  15,  73,  61,  61,  73,  16,   5,  88,  27,   4, 183,\n",
      "         49, 171,  61, 183,  16,  27, 216,  88,   1,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  15,  15,  73,  61,  61,  73,  16,   5,  88,  73,  86,   4,\n",
      "        183,  48, 171,  61, 183,  16,  27, 216,  88, 183,   1,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  15,  15,  86,  27,  16,  39,  88,   5,  88,  39,  73,  65,\n",
      "          4, 183,  48, 239, 194,  16, 216,  88,  27, 216, 260, 183,  65,   1,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  15,  48,  65,  73, 118,  55,  27,  16,  37,  27,   4, 183,\n",
      "         48,  65, 171,  55, 215,  25,   1,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  15,  48,  65,  73, 118,  55,  27,  16,  37,  27,   4, 183,\n",
      "        206,  65, 171,  55, 215,  25,   1,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  16,   5,  61,   4, 130, 253,  63,   1,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  16,   5,  61,   4, 130, 253, 183,  61,   1,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  16,  16,  27,  16,   4,   5,  16, 183,  16,   1,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  16,  16,  27,  16,   4,   5,  16, 216,  16,   1,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  16,  16,  27,  16,   4, 130, 253, 183,  16,   1,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  16,  16,  27,  16,   4, 130, 253, 216,  16,   1,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  16,  16,  39,  88,  39,  73,  65,   4, 183,  16, 216, 260,\n",
      "         70,   1,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  16,  27,  55,   5,  39,  16,  27,   4, 130,  16, 200,  55,\n",
      "        130, 225,  16,   1,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  16,  61,  39,  65,  39,  87,  88,  86,   5,  88,  39, 117,\n",
      "         27,   4, 183,  16,  61, 216,  65, 216,  87,  94, 246, 299, 183,  89,\n",
      "        216, 117,   1,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  27,  86,  73,  87,  27,   4,  39, 239, 183, 268, 123,   1,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  37,  37,  55, 109,  88,  39,  65,  39,  65,   4, 183, 206,\n",
      "         55, 111, 290,  70, 216,  65,   1,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  39,  37,  38,  88,   4,   5, 225, 290,   1,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  39,  37,  38,  88,   4, 183, 225, 290,   1,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  55,   5,  15,  86,  39,  88, 121,   4, 183,  55, 130,  48,\n",
      "        239, 213,  88,  39,   1,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([ 3,  5, 55,  9,  5, 65,  4,  5, 55,  9, 70,  1,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  2,  2,  2,  2]), tensor([  3,   5,  55,   9,  39,  15,   5,  65,  88,   4,   5,  55,   9, 216,\n",
      "         48,  70,  88,   1,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  55,   9, 109,  61,   4,   5, 226,  12,  63,   1,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  55,  16,  27,  86,   4, 178,  55,  16, 183,   1,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  55,  16,  27,  86,  87,   4, 178,  55,  16, 183, 123,   1,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  55,  37,  73,  86,  39,  88,  38,  61,   4, 130, 226, 206,\n",
      "        183, 239, 216, 144,  63,   1,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  55,  39,  27,  65,   5,  88,  39,  73,  65,   4,  27, 216,\n",
      "         55,  39, 183,  65,  27, 216, 260, 183,  65,   1,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  55,  39,  37,  65,  61,  27,  65,  88,   4, 183, 226,   5,\n",
      "        216,  65,  61, 183,  65,  88,   1,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  55,  55,   4, 178, 226,   1,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  55,  55,  27,  86,  37,  39,  27,  87,   4, 130,  55, 183,\n",
      "         25,  39, 123,   1,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  55,  55,  27,  86,  37,  39,  27,  87,   4, 130,  55, 190,\n",
      "         25,  39, 123,   1,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  55,  55,  27,  86,  37, 121,   4, 130,  55, 183,  16, 289,\n",
      "         39,   1,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  55,  55,  27,  86,  37, 121,   4, 130,  55, 190,  25,  39,\n",
      "          1,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  55,  61,   5,  65,   5,  15,   4, 178,  55,  61, 183,  65,\n",
      "        130,  48,   1,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  55,  77,  38,   5,   4, 130, 226,  33, 183,   1,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  55,  88,  27,  86,  65,   5,  88,  27,  55, 121,   4, 178,\n",
      "         55,  88, 200, 239,  65, 183,  88,  55, 216,   1,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([ 3,  5, 61,  4,  5, 61,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  2,  2,  2,  2]), tensor([  3,   5,  61,   4, 130,  61,   1,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  61,   5,  88,  73,  86, 121,   4, 130,  61, 183,  88, 177,\n",
      "        193,  39,   1,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  61,   5,  88,  73,  86, 121,   4, 165,  61, 183,  88, 183,\n",
      "        239, 216,   1,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  61,   5, 123,  39,  65,  37,   4, 183,  61,  27, 225, 123,\n",
      "        216, 155,   1,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  61,  77,  38,  39,   9,  39,  73, 109,  87,   4, 130,  61,\n",
      "         33, 216,   9,  39, 183,  87,   1,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  65,   4, 194, 183,  65,   1,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  65,   5,  55,  27,  15,  88,  87,   4, 130,  65, 183,  55,\n",
      "        194,  48,  88,  87,   1,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  65,  16,  86,  27, 118,   4,  32,  65,  19, 244, 299, 273,\n",
      "        109,   1,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  65,  16,  86,  27, 118,   4, 130,  65,  19, 245, 298, 273,\n",
      "        109,   1,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  65,  37,  87,  88,  87,   4,  27, 216, 155,  48,  87,  88,\n",
      "         87,   1,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  65,  88,  38,  86,  73,  77,  73,  61,  73,  86,  77,  38,\n",
      "         39,  15,   4, 135,  71, 307, 244, 183,  77, 183,  61, 177, 244,  33,\n",
      "        222,  48,   1,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  77,  38,  73,  86,  39,  87,  61,   4, 130,  33, 183, 239,\n",
      "        216, 123,  63,   1,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  77,  77,  55,  27,   4, 130,  77, 228,   1,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  77,  77,  55,  27,  16,   4, 130,  77, 228,  16,   1,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  86,   9,  39,  88,  86,   5,  88,  39,  73,  65,   4, 165,\n",
      "          9, 183,  97,  27, 216, 260,  70,   1,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  86,  37,  38,   4, 163, 209,   1,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  86,  37,  38,   4, 163, 239, 208,   1,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  86,  39,  87,  88,  73,  88,  55,  27,   4, 130, 239, 216,\n",
      "         87,  88, 171, 107, 228,   1,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  86,  55,  73,  65,   4, 163, 239, 226, 178,  65,   1,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  86,  55,  73,  65,   4, 165, 226, 178,  65,   1,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  86,  88,   4, 165,  88,   1,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  86,  88,  39,  15,  55,  27,   4, 165,  89, 216,  49, 183,\n",
      "        226,   1,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  88,  27,  65,  16,   4, 183,  89, 194,  65,  16,   1,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  88,  55,   5,  65,  88,   5,   4, 130,  88,  55, 130,  65,\n",
      "        183,   1,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  88,  55,   5,  65,  88,   5,   4, 130,  88,  55, 130, 255,\n",
      "        183,   1,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  88,  88,  27,  65,  16,   4, 183,  89, 194,  65,  16,   1,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  88,  88,  39,  15,   4, 130, 253, 216,  48,   1,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  88,  88,  86,   5,  15,  88,  39,  73,  65,   4, 183,  88,\n",
      "        239, 130,  48, 260, 222,  65,   1,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5,  88,  88,  86,   5,  15,  88,  39,  73,  65,   4, 183, 105,\n",
      "        239, 130,  48, 260, 222,  65,   1,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5, 109,  37, 109,  87,  88,   5,  65,   4, 178, 206, 280,  87,\n",
      "         88,  70,   1,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5, 109,  55,   5,   4, 178,  55,   5,   1,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5, 109,  65,  88,   4,   5,  65,  88,   1,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5, 109,  65,  88,   4,   7,  65,  88,   1,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5, 109,  65,  88,   4,  27, 183,  65,  88,   1,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5, 109,  65,  88,   4, 129,  65,  88,   1,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5, 109,  65,  88,   4, 130,  65,  88,   1,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5, 109,  65,  88,   4, 162,  65,  88,   1,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5, 109,  65,  88,   4, 165,  65,  88,   1,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5, 109,  65,  88,   4, 173,  65,  88,   1,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5, 109,  65,  88,   4, 194, 183,  65,  88,   1,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5, 117,   5,  39,  55,   5,   9,  39,  55,  39,  88, 121,   4,\n",
      "        183, 117,  27, 216,  55, 183,   9, 216,  55, 216,  88, 216,   1,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5, 118,   5,  86,   5,   4, 130, 118, 165, 239, 280,   1,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5, 118,   5,  86,   5,   4, 171, 118, 280, 239, 280,   1,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   5, 121, 109,  38,   4, 296, 216, 281,   1,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   9,   5,   5,   4,   9, 139, 139, 139, 139,   1,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   9,   5,   5,   4,   9, 140,   1,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   9,   5,   5,   4,   9, 165,   1,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([  3,   9,   5,   9,   9,  55,  27,  86,   4,   9, 130,   9, 183,  55,\n",
      "        183, 239,   1,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2]), tensor([ 3,  9,  5, 15, 48,  4,  9,  5, 48,  1,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  2,  2,  2,  2])]\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for ex in dataset:\n",
    "    pad_ex = ex + ([padtoken] * (max_len - len(ex)))\n",
    "    data.append(torch.tensor(encode(pad_ex), dtype=torch.long))\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "56d43bb2-61ed-49e1-a8d7-d589ffe5c99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  3,  16,  27,   5,  15,  27,  88, 121,  55,  88,  86,   5,  65,  87,\n",
      "         33,  27,  86,   5,  87,  27,   4,  16,  40, 183,  87,  40,  88,   5,\n",
      "        216, 226,  88, 239, 165,  65,  87,  33, 183, 239,  27, 216, 123,   1,\n",
      "          2])\n"
     ]
    }
   ],
   "source": [
    "print(data[380])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea65cb55-3e1a-46ec-9f93-0f77472dea65",
   "metadata": {},
   "source": [
    "The number of training examples is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ab907625-86a4-4467-9e27-df4267126340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3633\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee577833-20ba-4db4-8f50-02164de12e94",
   "metadata": {},
   "source": [
    "Next step is to split up the dataset into a training set and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c3ae05aa-a4ef-4f42-a06d-a897b6a08feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d718e3-63be-476f-993c-bbcfb2f57d1b",
   "metadata": {},
   "source": [
    "We set the `block_size` to `max_len-2` because we have added a endoftext and pad token to each example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "a1e5d8ab-439d-4175-901e-4bfcf6e2a712",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = max_len-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28056078-b827-44ed-9135-b282500ba3d1",
   "metadata": {},
   "source": [
    "Check that the input and output tensor lengths are correct by checking on the longest example in the dataset. `x` is the input tensor and `y` is the tensor with the output for each prefix of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a72afbf1-530f-4372-bd0c-7335df590c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  3,  16,  27,   5,  15,  27,  88, 121,  55,  88,  86,   5,  65,  87,\n",
      "          33,  27,  86,   5,  87,  27,   4,  16,  40, 183,  87,  40,  88,   5,\n",
      "         216, 226,  88, 239, 165,  65,  87,  33, 183, 239,  27, 216, 123]]) torch.Size([1, 41])\n",
      "tensor([[ 16,  27,   5,  15,  27,  88, 121,  55,  88,  86,   5,  65,  87,  33,\n",
      "          27,  86,   5,  87,  27,   4,  16,  40, 183,  87,  40,  88,   5, 216,\n",
      "         226,  88, 239, 165,  65,  87,  33, 183, 239,  27, 216, 123,   1]]) torch.Size([1, 41])\n"
     ]
    }
   ],
   "source": [
    "x = torch.stack([data[longest_idx][0:block_size]])\n",
    "y = torch.stack([data[longest_idx][1:block_size+1]])\n",
    "print(x, x.size())\n",
    "print(y, y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "7374aa95-3f28-4da4-9c43-1afc3613d750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  3,  15,  39,  86,  15,  55,  27,   4,  87, 201,  48, 183, 226,   1,\n",
      "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2],\n",
      "        [  3,  15, 109,  86,  87,  27,  16,   4,  49, 202,  87,  88,   1,   2,\n",
      "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2],\n",
      "        [  3,  39,  65,  16,  39,  15,   5,  55,   4, 216,  65,  16, 216,  48,\n",
      "          59,   1,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2],\n",
      "        [  3,  88, 118,  27,  55,  33,  88,  38,   4,  89, 119, 183,  56,  33,\n",
      "           1,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2]]) torch.Size([4, 41])\n",
      "tensor([[ 15,  39,  86,  15,  55,  27,   4,  87, 201,  48, 183, 226,   1,   2,\n",
      "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2],\n",
      "        [ 15, 109,  86,  87,  27,  16,   4,  49, 202,  87,  88,   1,   2,   2,\n",
      "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2],\n",
      "        [ 39,  65,  16,  39,  15,   5,  55,   4, 216,  65,  16, 216,  48,  59,\n",
      "           1,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2],\n",
      "        [ 88, 118,  27,  55,  33,  88,  38,   4,  89, 119, 183,  56,  33,   1,\n",
      "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2]]) torch.Size([4, 41])\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split, batch_size):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data), (batch_size,))\n",
    "    x = torch.stack([data[i][0:block_size] for i in ix])\n",
    "    y = torch.stack([data[i][1:block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "batch_size = 4\n",
    "xb, yb = get_batch('train', batch_size)\n",
    "print(xb, xb.size())\n",
    "print(yb, yb.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "efe1e721-c775-4bb9-b1e2-781ee6b4508f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is [3] the target is: 15\n",
      "when input is [3, 15] the target is: 39\n",
      "when input is [3, 15, 39] the target is: 86\n",
      "when input is [3, 15, 39, 86] the target is: 15\n",
      "when input is [3, 15, 39, 86, 15] the target is: 55\n",
      "when input is [3, 15, 39, 86, 15, 55] the target is: 27\n",
      "when input is [3, 15, 39, 86, 15, 55, 27] the target is: 4\n",
      "when input is [3, 15, 39, 86, 15, 55, 27, 4] the target is: 87\n",
      "when input is [3, 15, 39, 86, 15, 55, 27, 4, 87] the target is: 201\n",
      "when input is [3, 15, 39, 86, 15, 55, 27, 4, 87, 201] the target is: 48\n",
      "when input is [3, 15, 39, 86, 15, 55, 27, 4, 87, 201, 48] the target is: 183\n",
      "when input is [3, 15, 39, 86, 15, 55, 27, 4, 87, 201, 48, 183] the target is: 226\n",
      "when input is [3, 15, 39, 86, 15, 55, 27, 4, 87, 201, 48, 183, 226] the target is: 1\n",
      "when input is [3, 15, 39, 86, 15, 55, 27, 4, 87, 201, 48, 183, 226, 1] the target is: 2\n",
      "when input is [3, 15, 39, 86, 15, 55, 27, 4, 87, 201, 48, 183, 226, 1, 2] the target is: 2\n",
      "when input is [3, 15, 39, 86, 15, 55, 27, 4, 87, 201, 48, 183, 226, 1, 2, 2] the target is: 2\n",
      "when input is [3, 15, 39, 86, 15, 55, 27, 4, 87, 201, 48, 183, 226, 1, 2, 2, 2] the target is: 2\n",
      "when input is [3, 15, 39, 86, 15, 55, 27, 4, 87, 201, 48, 183, 226, 1, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 15, 39, 86, 15, 55, 27, 4, 87, 201, 48, 183, 226, 1, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 15, 39, 86, 15, 55, 27, 4, 87, 201, 48, 183, 226, 1, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 15, 39, 86, 15, 55, 27, 4, 87, 201, 48, 183, 226, 1, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 15, 39, 86, 15, 55, 27, 4, 87, 201, 48, 183, 226, 1, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 15, 39, 86, 15, 55, 27, 4, 87, 201, 48, 183, 226, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 15, 39, 86, 15, 55, 27, 4, 87, 201, 48, 183, 226, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 15, 39, 86, 15, 55, 27, 4, 87, 201, 48, 183, 226, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 15, 39, 86, 15, 55, 27, 4, 87, 201, 48, 183, 226, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 15, 39, 86, 15, 55, 27, 4, 87, 201, 48, 183, 226, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 15, 39, 86, 15, 55, 27, 4, 87, 201, 48, 183, 226, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 15, 39, 86, 15, 55, 27, 4, 87, 201, 48, 183, 226, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 15, 39, 86, 15, 55, 27, 4, 87, 201, 48, 183, 226, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 15, 39, 86, 15, 55, 27, 4, 87, 201, 48, 183, 226, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 15, 39, 86, 15, 55, 27, 4, 87, 201, 48, 183, 226, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 15, 39, 86, 15, 55, 27, 4, 87, 201, 48, 183, 226, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 15, 39, 86, 15, 55, 27, 4, 87, 201, 48, 183, 226, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 15, 39, 86, 15, 55, 27, 4, 87, 201, 48, 183, 226, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 15, 39, 86, 15, 55, 27, 4, 87, 201, 48, 183, 226, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 15, 39, 86, 15, 55, 27, 4, 87, 201, 48, 183, 226, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 15, 39, 86, 15, 55, 27, 4, 87, 201, 48, 183, 226, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 15, 39, 86, 15, 55, 27, 4, 87, 201, 48, 183, 226, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 15, 39, 86, 15, 55, 27, 4, 87, 201, 48, 183, 226, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 15, 39, 86, 15, 55, 27, 4, 87, 201, 48, 183, 226, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3] the target is: 15\n",
      "when input is [3, 15] the target is: 109\n",
      "when input is [3, 15, 109] the target is: 86\n",
      "when input is [3, 15, 109, 86] the target is: 87\n",
      "when input is [3, 15, 109, 86, 87] the target is: 27\n",
      "when input is [3, 15, 109, 86, 87, 27] the target is: 16\n",
      "when input is [3, 15, 109, 86, 87, 27, 16] the target is: 4\n",
      "when input is [3, 15, 109, 86, 87, 27, 16, 4] the target is: 49\n",
      "when input is [3, 15, 109, 86, 87, 27, 16, 4, 49] the target is: 202\n",
      "when input is [3, 15, 109, 86, 87, 27, 16, 4, 49, 202] the target is: 87\n",
      "when input is [3, 15, 109, 86, 87, 27, 16, 4, 49, 202, 87] the target is: 88\n",
      "when input is [3, 15, 109, 86, 87, 27, 16, 4, 49, 202, 87, 88] the target is: 1\n",
      "when input is [3, 15, 109, 86, 87, 27, 16, 4, 49, 202, 87, 88, 1] the target is: 2\n",
      "when input is [3, 15, 109, 86, 87, 27, 16, 4, 49, 202, 87, 88, 1, 2] the target is: 2\n",
      "when input is [3, 15, 109, 86, 87, 27, 16, 4, 49, 202, 87, 88, 1, 2, 2] the target is: 2\n",
      "when input is [3, 15, 109, 86, 87, 27, 16, 4, 49, 202, 87, 88, 1, 2, 2, 2] the target is: 2\n",
      "when input is [3, 15, 109, 86, 87, 27, 16, 4, 49, 202, 87, 88, 1, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 15, 109, 86, 87, 27, 16, 4, 49, 202, 87, 88, 1, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 15, 109, 86, 87, 27, 16, 4, 49, 202, 87, 88, 1, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 15, 109, 86, 87, 27, 16, 4, 49, 202, 87, 88, 1, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 15, 109, 86, 87, 27, 16, 4, 49, 202, 87, 88, 1, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 15, 109, 86, 87, 27, 16, 4, 49, 202, 87, 88, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 15, 109, 86, 87, 27, 16, 4, 49, 202, 87, 88, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 15, 109, 86, 87, 27, 16, 4, 49, 202, 87, 88, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 15, 109, 86, 87, 27, 16, 4, 49, 202, 87, 88, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 15, 109, 86, 87, 27, 16, 4, 49, 202, 87, 88, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 15, 109, 86, 87, 27, 16, 4, 49, 202, 87, 88, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 15, 109, 86, 87, 27, 16, 4, 49, 202, 87, 88, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 15, 109, 86, 87, 27, 16, 4, 49, 202, 87, 88, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 15, 109, 86, 87, 27, 16, 4, 49, 202, 87, 88, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 15, 109, 86, 87, 27, 16, 4, 49, 202, 87, 88, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 15, 109, 86, 87, 27, 16, 4, 49, 202, 87, 88, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 15, 109, 86, 87, 27, 16, 4, 49, 202, 87, 88, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 15, 109, 86, 87, 27, 16, 4, 49, 202, 87, 88, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 15, 109, 86, 87, 27, 16, 4, 49, 202, 87, 88, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 15, 109, 86, 87, 27, 16, 4, 49, 202, 87, 88, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 15, 109, 86, 87, 27, 16, 4, 49, 202, 87, 88, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 15, 109, 86, 87, 27, 16, 4, 49, 202, 87, 88, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 15, 109, 86, 87, 27, 16, 4, 49, 202, 87, 88, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 15, 109, 86, 87, 27, 16, 4, 49, 202, 87, 88, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 15, 109, 86, 87, 27, 16, 4, 49, 202, 87, 88, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3] the target is: 39\n",
      "when input is [3, 39] the target is: 65\n",
      "when input is [3, 39, 65] the target is: 16\n",
      "when input is [3, 39, 65, 16] the target is: 39\n",
      "when input is [3, 39, 65, 16, 39] the target is: 15\n",
      "when input is [3, 39, 65, 16, 39, 15] the target is: 5\n",
      "when input is [3, 39, 65, 16, 39, 15, 5] the target is: 55\n",
      "when input is [3, 39, 65, 16, 39, 15, 5, 55] the target is: 4\n",
      "when input is [3, 39, 65, 16, 39, 15, 5, 55, 4] the target is: 216\n",
      "when input is [3, 39, 65, 16, 39, 15, 5, 55, 4, 216] the target is: 65\n",
      "when input is [3, 39, 65, 16, 39, 15, 5, 55, 4, 216, 65] the target is: 16\n",
      "when input is [3, 39, 65, 16, 39, 15, 5, 55, 4, 216, 65, 16] the target is: 216\n",
      "when input is [3, 39, 65, 16, 39, 15, 5, 55, 4, 216, 65, 16, 216] the target is: 48\n",
      "when input is [3, 39, 65, 16, 39, 15, 5, 55, 4, 216, 65, 16, 216, 48] the target is: 59\n",
      "when input is [3, 39, 65, 16, 39, 15, 5, 55, 4, 216, 65, 16, 216, 48, 59] the target is: 1\n",
      "when input is [3, 39, 65, 16, 39, 15, 5, 55, 4, 216, 65, 16, 216, 48, 59, 1] the target is: 2\n",
      "when input is [3, 39, 65, 16, 39, 15, 5, 55, 4, 216, 65, 16, 216, 48, 59, 1, 2] the target is: 2\n",
      "when input is [3, 39, 65, 16, 39, 15, 5, 55, 4, 216, 65, 16, 216, 48, 59, 1, 2, 2] the target is: 2\n",
      "when input is [3, 39, 65, 16, 39, 15, 5, 55, 4, 216, 65, 16, 216, 48, 59, 1, 2, 2, 2] the target is: 2\n",
      "when input is [3, 39, 65, 16, 39, 15, 5, 55, 4, 216, 65, 16, 216, 48, 59, 1, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 39, 65, 16, 39, 15, 5, 55, 4, 216, 65, 16, 216, 48, 59, 1, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 39, 65, 16, 39, 15, 5, 55, 4, 216, 65, 16, 216, 48, 59, 1, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 39, 65, 16, 39, 15, 5, 55, 4, 216, 65, 16, 216, 48, 59, 1, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 39, 65, 16, 39, 15, 5, 55, 4, 216, 65, 16, 216, 48, 59, 1, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 39, 65, 16, 39, 15, 5, 55, 4, 216, 65, 16, 216, 48, 59, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 39, 65, 16, 39, 15, 5, 55, 4, 216, 65, 16, 216, 48, 59, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 39, 65, 16, 39, 15, 5, 55, 4, 216, 65, 16, 216, 48, 59, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 39, 65, 16, 39, 15, 5, 55, 4, 216, 65, 16, 216, 48, 59, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 39, 65, 16, 39, 15, 5, 55, 4, 216, 65, 16, 216, 48, 59, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 39, 65, 16, 39, 15, 5, 55, 4, 216, 65, 16, 216, 48, 59, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 39, 65, 16, 39, 15, 5, 55, 4, 216, 65, 16, 216, 48, 59, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 39, 65, 16, 39, 15, 5, 55, 4, 216, 65, 16, 216, 48, 59, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 39, 65, 16, 39, 15, 5, 55, 4, 216, 65, 16, 216, 48, 59, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 39, 65, 16, 39, 15, 5, 55, 4, 216, 65, 16, 216, 48, 59, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 39, 65, 16, 39, 15, 5, 55, 4, 216, 65, 16, 216, 48, 59, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 39, 65, 16, 39, 15, 5, 55, 4, 216, 65, 16, 216, 48, 59, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 39, 65, 16, 39, 15, 5, 55, 4, 216, 65, 16, 216, 48, 59, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 39, 65, 16, 39, 15, 5, 55, 4, 216, 65, 16, 216, 48, 59, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 39, 65, 16, 39, 15, 5, 55, 4, 216, 65, 16, 216, 48, 59, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 39, 65, 16, 39, 15, 5, 55, 4, 216, 65, 16, 216, 48, 59, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 39, 65, 16, 39, 15, 5, 55, 4, 216, 65, 16, 216, 48, 59, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3] the target is: 88\n",
      "when input is [3, 88] the target is: 118\n",
      "when input is [3, 88, 118] the target is: 27\n",
      "when input is [3, 88, 118, 27] the target is: 55\n",
      "when input is [3, 88, 118, 27, 55] the target is: 33\n",
      "when input is [3, 88, 118, 27, 55, 33] the target is: 88\n",
      "when input is [3, 88, 118, 27, 55, 33, 88] the target is: 38\n",
      "when input is [3, 88, 118, 27, 55, 33, 88, 38] the target is: 4\n",
      "when input is [3, 88, 118, 27, 55, 33, 88, 38, 4] the target is: 89\n",
      "when input is [3, 88, 118, 27, 55, 33, 88, 38, 4, 89] the target is: 119\n",
      "when input is [3, 88, 118, 27, 55, 33, 88, 38, 4, 89, 119] the target is: 183\n",
      "when input is [3, 88, 118, 27, 55, 33, 88, 38, 4, 89, 119, 183] the target is: 56\n",
      "when input is [3, 88, 118, 27, 55, 33, 88, 38, 4, 89, 119, 183, 56] the target is: 33\n",
      "when input is [3, 88, 118, 27, 55, 33, 88, 38, 4, 89, 119, 183, 56, 33] the target is: 1\n",
      "when input is [3, 88, 118, 27, 55, 33, 88, 38, 4, 89, 119, 183, 56, 33, 1] the target is: 2\n",
      "when input is [3, 88, 118, 27, 55, 33, 88, 38, 4, 89, 119, 183, 56, 33, 1, 2] the target is: 2\n",
      "when input is [3, 88, 118, 27, 55, 33, 88, 38, 4, 89, 119, 183, 56, 33, 1, 2, 2] the target is: 2\n",
      "when input is [3, 88, 118, 27, 55, 33, 88, 38, 4, 89, 119, 183, 56, 33, 1, 2, 2, 2] the target is: 2\n",
      "when input is [3, 88, 118, 27, 55, 33, 88, 38, 4, 89, 119, 183, 56, 33, 1, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 88, 118, 27, 55, 33, 88, 38, 4, 89, 119, 183, 56, 33, 1, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 88, 118, 27, 55, 33, 88, 38, 4, 89, 119, 183, 56, 33, 1, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 88, 118, 27, 55, 33, 88, 38, 4, 89, 119, 183, 56, 33, 1, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 88, 118, 27, 55, 33, 88, 38, 4, 89, 119, 183, 56, 33, 1, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 88, 118, 27, 55, 33, 88, 38, 4, 89, 119, 183, 56, 33, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 88, 118, 27, 55, 33, 88, 38, 4, 89, 119, 183, 56, 33, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 88, 118, 27, 55, 33, 88, 38, 4, 89, 119, 183, 56, 33, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 88, 118, 27, 55, 33, 88, 38, 4, 89, 119, 183, 56, 33, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 88, 118, 27, 55, 33, 88, 38, 4, 89, 119, 183, 56, 33, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 88, 118, 27, 55, 33, 88, 38, 4, 89, 119, 183, 56, 33, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 88, 118, 27, 55, 33, 88, 38, 4, 89, 119, 183, 56, 33, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 88, 118, 27, 55, 33, 88, 38, 4, 89, 119, 183, 56, 33, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 88, 118, 27, 55, 33, 88, 38, 4, 89, 119, 183, 56, 33, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 88, 118, 27, 55, 33, 88, 38, 4, 89, 119, 183, 56, 33, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 88, 118, 27, 55, 33, 88, 38, 4, 89, 119, 183, 56, 33, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 88, 118, 27, 55, 33, 88, 38, 4, 89, 119, 183, 56, 33, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 88, 118, 27, 55, 33, 88, 38, 4, 89, 119, 183, 56, 33, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 88, 118, 27, 55, 33, 88, 38, 4, 89, 119, 183, 56, 33, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 88, 118, 27, 55, 33, 88, 38, 4, 89, 119, 183, 56, 33, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 88, 118, 27, 55, 33, 88, 38, 4, 89, 119, 183, 56, 33, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 88, 118, 27, 55, 33, 88, 38, 4, 89, 119, 183, 56, 33, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n",
      "when input is [3, 88, 118, 27, 55, 33, 88, 38, 4, 89, 119, 183, 56, 33, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] the target is: 2\n"
     ]
    }
   ],
   "source": [
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f'when input is {context.tolist()} the target is: {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74bca45-73a7-4fd2-80f7-d56b0e1b475a",
   "metadata": {},
   "source": [
    "## Bigram Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b30fe70-731e-44bc-b87b-c3d613bf137b",
   "metadata": {},
   "source": [
    "Let us define $B$ as the `batch_size`, $T$ is the time dimension aka the length of the input sequence, and $C$ is the so-called channel size which is defined to be the `vocab_size` (the output dimension).\n",
    "\n",
    "The bigram language model (LM) is defined here to be a very simple LM where we use an embedding for the current token which is define to be size `vocab_size` to predict the next token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "58347111-e5e1-4811-b981-84cabb87bd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token has an embedding which predicts the next token\n",
    "        self.vocab_size = vocab_size\n",
    "        self.bigram = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets):\n",
    "        # B: batch_size\n",
    "        # T: time\n",
    "        # C: channel_size aka vocab_size\n",
    "        # idx and targets are (B,T) size tensors\n",
    "        logits = self.bigram(idx) # returns (B,T,C) tensor\n",
    "        B, T, C = logits.shape\n",
    "        logits = logits.view(B*T, self.vocab_size)\n",
    "        loss = F.cross_entropy(logits, targets.view(B*T))\n",
    "        #print(logits.view(B*T, self.vocab_size).shape)\n",
    "        #print(targets.view(B*T).shape)\n",
    "        # note that this is the same as the following using Pytorch type coercion:\n",
    "        # loss = F.cross_entropy(logits.view(-1, self.vocab_size), targets.view(-1))\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "311922d4-ca8f-4b04-bffd-515fd52446b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size=312\n"
     ]
    }
   ],
   "source": [
    "print(f'{vocab_size=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "e6359b95-80fb-4859-bc5f-c459ae1191dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7827, -1.0718, -0.7268,  ..., -1.6196,  0.2838,  0.9324],\n",
      "        [-0.9436, -2.1403, -0.8627,  ..., -1.2252, -1.3771, -0.6452],\n",
      "        [ 1.9684,  0.8390,  0.6633,  ..., -0.8454, -0.4605,  1.8370],\n",
      "        ...,\n",
      "        [-0.2203, -0.4863,  1.3640,  ..., -1.4011, -1.1136, -1.1035],\n",
      "        [-0.2203, -0.4863,  1.3640,  ..., -1.4011, -1.1136, -1.1035],\n",
      "        [-0.2203, -0.4863,  1.3640,  ..., -1.4011, -1.1136, -1.1035]],\n",
      "       grad_fn=<ViewBackward0>) torch.Size([164, 312])\n",
      "tensor(5.3761, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model1 = BigramLM(vocab_size)\n",
    "out, loss = model1(xb, yb)\n",
    "print(out, out.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3813c222-8b08-4718-b95e-cd42cc2f2234",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc81fc39-c0f1-409b-80a2-c02bb172879b",
   "metadata": {},
   "source": [
    "We expect loss to be $\\log_e(\\frac{1}{vocabsize})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "245f03af-8d2e-4445-80cb-b2ab82f3ec68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-5.7430)\n"
     ]
    }
   ],
   "source": [
    "print(torch.log(torch.tensor(1/vocab_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07d255a-d621-4c15-82c2-ddd5388ecabf",
   "metadata": {},
   "source": [
    "### Add a generator function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "73a03bdd-cdc2-4417-b673-7c9ef7395599",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token has an embedding which predicts the next token\n",
    "        self.vocab_size = vocab_size\n",
    "        self.bigram = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        # B: batch_size\n",
    "        # T: time\n",
    "        # C: channel_size aka vocab_size\n",
    "        # idx is a (B,T) size tensor\n",
    "        return self.bigram(idx) # returns (B,T,C) tensor\n",
    "\n",
    "    def loss(self, logits, targets):\n",
    "        # logits is a (B, T, C) tensor\n",
    "        # target is a (B, T) size tensor\n",
    "        return F.cross_entropy(\n",
    "                logits.view(-1, self.vocab_size), # convert to (B*T, C) tensor\n",
    "                targets.view(-1) # convert to (B*T) tensor\n",
    "                )\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is a (B, T) size tensor of indices\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits = self(idx) # ignore loss\n",
    "            # use the last time step\n",
    "            logits = logits[:, -1, :] # which is size (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # size (B, C)\n",
    "            # sample from this probability distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # size (B, 1)\n",
    "            # append sampled token to sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # size (B, T+1)\n",
    "            decoded = decode(idx[-1].tolist())\n",
    "            if decoded[-1] == '<|endoftext|>':\n",
    "                break\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "a51ea710-4018-4832-8fe6-48eefad22400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits=tensor([[[ 0.6713, -0.8438, -0.2006,  ..., -2.5943, -1.8699,  0.0832],\n",
      "         [ 2.7900,  0.4559, -0.7600,  ..., -1.5182, -2.6879,  0.4875],\n",
      "         [ 0.0732, -2.0100, -1.0449,  ..., -0.4761, -0.0779,  0.2565],\n",
      "         ...,\n",
      "         [-0.1549, -0.8079, -1.4545,  ..., -1.7372, -1.2094, -0.9094],\n",
      "         [-0.1549, -0.8079, -1.4545,  ..., -1.7372, -1.2094, -0.9094],\n",
      "         [-0.1549, -0.8079, -1.4545,  ..., -1.7372, -1.2094, -0.9094]],\n",
      "\n",
      "        [[ 0.6713, -0.8438, -0.2006,  ..., -2.5943, -1.8699,  0.0832],\n",
      "         [ 2.7900,  0.4559, -0.7600,  ..., -1.5182, -2.6879,  0.4875],\n",
      "         [ 0.3202, -0.4898,  2.0381,  ...,  0.2875,  1.5958,  1.2748],\n",
      "         ...,\n",
      "         [-0.1549, -0.8079, -1.4545,  ..., -1.7372, -1.2094, -0.9094],\n",
      "         [-0.1549, -0.8079, -1.4545,  ..., -1.7372, -1.2094, -0.9094],\n",
      "         [-0.1549, -0.8079, -1.4545,  ..., -1.7372, -1.2094, -0.9094]],\n",
      "\n",
      "        [[ 0.6713, -0.8438, -0.2006,  ..., -2.5943, -1.8699,  0.0832],\n",
      "         [ 0.0732, -2.0100, -1.0449,  ..., -0.4761, -0.0779,  0.2565],\n",
      "         [ 0.1231,  0.6471, -2.2113,  ...,  1.9579,  0.9306, -1.4730],\n",
      "         ...,\n",
      "         [-0.1549, -0.8079, -1.4545,  ..., -1.7372, -1.2094, -0.9094],\n",
      "         [-0.1549, -0.8079, -1.4545,  ..., -1.7372, -1.2094, -0.9094],\n",
      "         [-0.1549, -0.8079, -1.4545,  ..., -1.7372, -1.2094, -0.9094]],\n",
      "\n",
      "        [[ 0.6713, -0.8438, -0.2006,  ..., -2.5943, -1.8699,  0.0832],\n",
      "         [-0.8545,  1.1486, -0.2845,  ..., -0.1263, -0.4914, -2.1440],\n",
      "         [-0.1351,  0.2206, -0.2953,  ...,  1.2686, -1.3927, -1.0164],\n",
      "         ...,\n",
      "         [-0.1549, -0.8079, -1.4545,  ..., -1.7372, -1.2094, -0.9094],\n",
      "         [-0.1549, -0.8079, -1.4545,  ..., -1.7372, -1.2094, -0.9094],\n",
      "         [-0.1549, -0.8079, -1.4545,  ..., -1.7372, -1.2094, -0.9094]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "logits.shape=torch.Size([4, 41, 312])\n",
      "loss=tensor(7.1260, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model1 = BigramLM(vocab_size)\n",
    "logits = model1(xb)\n",
    "loss = model1.loss(logits, yb)\n",
    "print(f'{logits=}')\n",
    "print(f'{logits.shape=}')\n",
    "print(f'{loss=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "ad3407f5-38b2-4514-8598-e4cae83c0f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3])"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(encode(['<|starttext|>']), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14ad70b-f9ae-4f41-8c84-e27175d6a503",
   "metadata": {},
   "source": [
    "We need a batch dimension even if the batch size for generation is just one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "7497632b-c433-44ed-9e87-2dbd5119612c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3]])"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = torch.eye(1, dtype=torch.long)*torch.tensor(encode(['<|starttext|>']), dtype=torch.long)\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "50823bec-f569-490c-a5b9-5124b1c34126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  3, 191, 257, 291,  50, 209, 217, 308,  36,  85,  98,  83, 182, 193,\n",
       "         302, 259, 164, 239, 149, 131, 200,  24,   5, 288, 285, 305, 293, 287,\n",
       "         275,  90, 185, 118, 129, 106, 189, 203, 249,  72, 106, 202, 220, 250,\n",
       "         291,  95, 280, 126, 126, 286, 289, 242,  59, 288,  26,  10,  92, 222,\n",
       "          14, 270,  78, 115, 111,  25,   2, 165,  23, 202, 116, 161, 209,  52,\n",
       "         179, 241,  88,  17, 187,  44, 117, 235,  57,  20, 268, 205, 284, 227,\n",
       "          59,  21, 251, 177, 200, 113, 162, 129,  22,  61,  70,  66, 253, 153,\n",
       "          59, 201,  83]])"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.generate(idx, max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1229882b-c001-416c-94d8-cd361837ae29",
   "metadata": {},
   "source": [
    "To extract the single element in this batch of size one we take the first element and convert it to a list before we decode it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "3d915fcd-2b28-43fe-964d-73272a203efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|starttext|>', 'ɪʰ', 'ɽ̃', 'd̠', 'ɹ̥', 'n̠', 'ʙ̥͡ɸː', 't̠͡ɹ̠̊', 'b̥', 'u̠ː', 'ɪ̃ː', 'æːː', 'ɐː', 'ǀ', 'w̥', 'ɤ', 'pʷ', 'ã', 'u̟', 'ʊː', 'd͡ɹ̝', 't', 'eᵊ', 'ʔ', 'ă', 'p̚', 'č', 'ɯ᷈ː', 'ɹ̝̊', 'l̥ʰ', 'œː', 'u', 'æ̝', 'd̚', 'z̥', 'ɪ̞', 'ʙ̥͡ɸː', 'ɑ̃', 'u̠ː', 'd̚', 'u̟', 'iː', 'ẽˑ', 'ɪ̥', 'n̥', 'ð', 't̬', 'z̥', 'ä', 'ʊ̯ˀ', 'ẽː', 'æ̝ː', 'ǀ', 'n̩', 'u', 'x', 'd̠͡ɹ̠', 'œː', 'ã', 'ɪˑ', 'ə̥', 'χ', 'e̞', 'm̩', 'ə̃', 'ɒ̃ⁿ', 'k͡s', 't̰̚', 'm̩ː', 'dⁿ', 'ɱ', 'ŋ̍', 'ŋ', 'n̩', 'æ̝', 'ɦ', 'ɑ̃ː', 'q', 'o', 'əː', 't͡ʃ', 'ð', 'yː', 'əː', 'ɑ̟', 'ʊ̯̃', 'jː', 'ɒ͡ɪ', 'ʊː', 'ɫ̥', 'd̥̚', 'ɾ̪̊', 'ɫ̥', '<|endoftext|>']\n"
     ]
    }
   ],
   "source": [
    "print(decode(model1.generate(idx, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef82ed59-3f17-4c08-afcd-08fe637a174c",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "id": "54d584da-0d5c-440d-8036-98179d736f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigramLM(\n",
      "  (bigram): Embedding(312, 312)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model1 = BigramLM(vocab_size=vocab_size)\n",
    "optimizer1 = optim.Adam(model1.parameters(), lr=1e-3)\n",
    "batch_size = 32\n",
    "losses = []\n",
    "validation_losses = []\n",
    "print(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "id": "22a6403a-a9a4-4ae4-9b10-0252b3eb041f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=0.9761354327201843\n"
     ]
    }
   ],
   "source": [
    "for _ in range(1000):\n",
    "    xb, yb = get_batch('train', batch_size)\n",
    "    logits = model1(xb)\n",
    "    loss = model1.loss(logits, yb)\n",
    "    losses.append(loss.log10().item())\n",
    "    #print(f'{loss=}')\n",
    "    optimizer1.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer1.step()\n",
    "\n",
    "print(f'loss={loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dfe433-63c7-4c47-9384-dc448b4b5fcb",
   "metadata": {},
   "source": [
    "### Calculate validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "id": "b84655c0-6a01-45b1-a7fa-12a136043981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0113, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch('val', batch_size)\n",
    "logits = model1(xb)\n",
    "val_loss = model1.loss(logits, yb)\n",
    "print(val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9b058c-7676-48a9-9c10-fa565350dc5c",
   "metadata": {},
   "source": [
    "### Plot the average loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "id": "b04ebc8e-5373-4545-b6ac-f809743b47a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1689e2f80>]"
      ]
     },
     "execution_count": 642,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABFU0lEQVR4nO3de1xUZeIG8GcuzAzX4T5cHEVUVLyAIiCaab8oK7N7UWm4WLaZuRq7bVKpXdawrVwrLYs0LTPNVrtoaS7dk0RBvOItRRAZLiIzXGdg5vz+oMZIUAaBM8M838/nfNo9vGfmobPrPM55z3skgiAIICIiIhKJVOwARERE5NxYRoiIiEhULCNEREQkKpYRIiIiEhXLCBEREYmKZYSIiIhExTJCREREomIZISIiIlHJxQ7QHhaLBWfPnoWnpyckEonYcYiIiKgdBEFAdXU1QkJCIJW2/f2HQ5SRs2fPQqvVih2DiIiIOqCoqAi9evVq8+cOUUY8PT0BNP8yXl5eIqchIiKi9jAYDNBqtdbP8bY4RBn5/dKMl5cXywgREZGDudwUC05gJSIiIlGxjBAREZGoWEaIiIhIVCwjREREJCqWESIiIhIVywgRERGJimWEiIiIRMUyQkRERKJiGSEiIiJRsYwQERGRqFhGiIiISFQsI0RERCQqpy4jPxwrx0Nr9qDeZBY7ChERkdNy2jJSa2zC3A15+F9+KVJWZ6PW2CR2JCIiIqfktGXEXSnH2w/EwEMpxy8nK5G8KhuGhkaxYxERETmdDpWR5cuXIywsDCqVCvHx8cjOzr7k+KVLl2LgwIFwdXWFVqvF448/joaGhg4F7kyxYb5Y+1A8vFRy5Jw+j6nv7kJVnUnsWERERE7F5jKyYcMGpKamYuHChcjNzUVUVBQmTpyIsrKyVsevW7cO8+bNw8KFC5Gfn4+VK1diw4YNeOqpp644fGeI1npj3YzR8HFzwf4zetyXsQvnaoxixyIiInIaNpeRJUuWYMaMGUhJSUFkZCRWrFgBNzc3rFq1qtXxO3fuxNixY3H//fcjLCwM119/Pe67777LfpvSnYaGqrHhrwnw91Aiv8SAe9/5BWXV4n9zQ0RE5AxsKiMmkwk5OTlITEy88AJSKRITE5GVldXqMWPGjEFOTo61fJw8eRJffvklbrrppjbfx2g0wmAwtNi6WoTGEx//dTSCvFQ4XlaDKRm7UMFvSIiIiLqcTWWkoqICZrMZGo2mxX6NRgOdTtfqMffffz+ef/55XHXVVXBxcUG/fv0wYcKES16mSU9Ph1qttm5ardaWmB0WHuCBDX8oJFPf3YXKWs4hISIi6kpdfjfNd999hxdffBFvvvkmcnNzsWnTJmzduhUvvPBCm8ekpaVBr9dbt6Kioq6OadXHzx0fPTwagZ5KHNFVc1IrERFRF7OpjPj7+0Mmk6G0tLTF/tLSUgQFBbV6zPz58/HAAw/goYcewrBhw3D77bfjxRdfRHp6OiwWS6vHKJVKeHl5tdi6U19/d6ybMRr+HkocLjHggZXZ0Nfztl8iIqKuYFMZUSgUiImJQWZmpnWfxWJBZmYmEhISWj2mrq4OUmnLt5HJZAAAQRBszdtt+gd6YN2MePi5K3CgWM91SIiIiLqIzZdpUlNTkZGRgTVr1iA/Px8zZ85EbW0tUlJSAADJyclIS0uzjp88eTLeeustrF+/HqdOncKOHTswf/58TJ482VpK7FWExhNrH4qHt5sL9hVVYdqqbFSzkBAREXUqua0HJCUloby8HAsWLIBOp0N0dDS2bdtmndRaWFjY4puQZ555BhKJBM888wyKi4sREBCAyZMnY9GiRZ33W3ShwcFeWPtgPKa8uwt7C6uQ8t5urJ4eBw+lzf/qiIiIqBUSwZ6vlfzGYDBArVZDr9d3+/yR3x04o8eUd3+BoaEJcWG+WD09Fm4KFhIiIqK2tPfz22mfTWOrYb3U+ODBeHgq5cguqMT01bv5tF8iIqJOwDJigyitN95/MM76cL0H17CQEBERXSmWERuN6O2DNdNj4a6QYeev5/DI2hyYmlq/RZmIiIguj2WkA2L6+GL19Di4usjw/bFyPL4hD2aL3U+9ISIiskssIx0UG+aLtx+IgUImxdYDJUjbtB8WFhIiIiKbsYxcgasjAvD6fdGQSoCP95zBv7bm2/VCbkRERPaIZeQK3TA0GP++KwoAsOrnU1j6v+MiJyIiInIsLCOd4K6YXnjuliEAgNcyj+PdH0+KnIiIiMhxsIx0kmljwvCP6yMAAP/amo9Pcs6InIiIiMgxsIx0olnX9MeMcX0BAE/+dz+2H9KJnIiIiMj+sYx0IolEgqduGoy7Y3rBbBEwe91e7DxRIXYsIiIiu8Yy0skkEgnS7xiGiUM0MJktmPH+HuwrqhI7FhERkd1iGekCcpkUr907AmP7+6HWZMZf3svG8dJqsWMRERHZJZaRLqJykeHtB0Yhqpca5+sakbwqG8VV9WLHIiIisjssI13IQynH6pQ49A/0QIm+Ackrd6Gy1iR2LCIiIrvCMtLFfNwVeH96HILVKvxaXouU1btRa2wSOxYREZHdYBnpBiHervjgwTj4uLlgX1EVn/RLRET0Bywj3aR/oCfeS4mDm0KGH49X4O8b9/HBekRERGAZ6VbRWm+smBoDF5kEX+w7i+e3HOaD9YiIyOmxjHSzqyMC8Oo90QCA1TsL8PYPfI4NERE5N5YREdwSFYL5N0cCABZ/dQSb9/I5NkRE5LxYRkTy4FV9rc+xeWLjfvx4vFzkREREROJgGRFR2o2DMTkqBE0WAY98kIODxXqxIxEREXU7lhERSaUSvHL3cCSENy8bn7J6N4oq68SORURE1K1YRkSmlMvwdnIMBgV5orzaiGnvZeM8V2klIiInwjJiB7xULlidEocQtQony2sx4/09aGg0ix2LiIioW7CM2IkgtQqrp8fBUyXHntPn8fePuSgaERE5B5YROxKh8cTbDzQvirb1QAnSv8oXOxIREVGXYxmxM2P6+eOVu6MAABk/nsLqn0+JnIiIiKhrsYzYoVujQ/HPGwYCAJ7bchjbDupETkRERNR1WEbs1Mzx/TB1dG8IAjB3w16uQUJERD1Wh8rI8uXLERYWBpVKhfj4eGRnZ7c5dsKECZBIJBdtkyZN6nBoZyCRSPDs5CEYHxGAhkYLZry/B2XVDWLHIiIi6nQ2l5ENGzYgNTUVCxcuRG5uLqKiojBx4kSUlZW1On7Tpk0oKSmxbgcPHoRMJsPdd999xeF7OrlMijfuH4F+Ae4o0Tfg4fdzeMsvERH1ODaXkSVLlmDGjBlISUlBZGQkVqxYATc3N6xatarV8b6+vggKCrJuO3bsgJubG8tIO3mpXPDutFioXV2QV1SFtE0HIAi85ZeIiHoOm8qIyWRCTk4OEhMTL7yAVIrExERkZWW16zVWrlyJe++9F+7u7m2OMRqNMBgMLTZn1tffHW9NGQmZVILNe4ux4vuTYkciIiLqNDaVkYqKCpjNZmg0mhb7NRoNdLrL3/GRnZ2NgwcP4qGHHrrkuPT0dKjVauum1Wptidkjjenvj2dvGQIA+Pf2I9hxuFTkRERERJ2jW++mWblyJYYNG4a4uLhLjktLS4Ner7duRUVF3ZTQvj0wug8eGN2n+Q6b9XtxROfc3xgREVHPYFMZ8ff3h0wmQ2lpy7+Vl5aWIigo6JLH1tbWYv369XjwwQcv+z5KpRJeXl4tNmq2YHIkxvRrfsrvQ2v24FyNUexIREREV8SmMqJQKBATE4PMzEzrPovFgszMTCQkJFzy2I0bN8JoNGLq1KkdS0oAABeZFG9OGYk+fm44c74eM9fmwtRkETsWERFRh9l8mSY1NRUZGRlYs2YN8vPzMXPmTNTW1iIlJQUAkJycjLS0tIuOW7lyJW677Tb4+fldeWon5+2mwMppo+CplCO7oBLzPz3IO2yIiMhhyW09ICkpCeXl5ViwYAF0Oh2io6Oxbds266TWwsJCSKUtO87Ro0fx008/4euvv+6c1IT+gZ54/f4ReHD1bmzYU4SBQZ6YflVfsWMRERHZTCI4wF+pDQYD1Go19Ho954/8ybs/nsS/tuZDKgHeS4nD+IgAsSMREREBaP/nN59N4+AevKov7hnVCxYBeGxdLk5V1IodiYiIyCYsIw5OIpHghduGYlQfH1Q3NOHh9/egxtgkdiwiIqJ2YxnpAZRyGd6cOhIaLyWOl9Xg7x/nwWKx+6tvREREAFhGeoxATxVWTI2BQibF9kOlWP7tCbEjERERtQvLSA8yorcPXritecn4Jf87hsx8LhlPRET2j2Wkh0mK7f2HJePz8Gt5jdiRiIiILollpAeaf3MkYsN8UG1sntBa3dAodiQiIqI2sYz0QAq5FG9OiUGQlwq/ltfi7x/v44RWIiKyWywjPVSApxIrHmie0Pr1YU5oJSIi+8Uy0oNFa71bTGj99kiZyImIiIguxjLSwyXF9saU+N4QBOBv6/eigCu0EhGRnWEZcQILJw9BzO8rtH6wB7VcoZWIiOwIy4gTUMileGvKSAR6KnGstAZPfLIPDvB8RCIichIsI04i0EuFt6aOhItMgi8P6LDq5wKxIxEREQFgGXEqMX188cykSABA+pf5yC08L3IiIiIilhGnk5zQB5OGBaPJIuCxD3NxvtYkdiQiInJyLCNORiKRYPGdw9DX3x1n9Q1I5RN+iYhIZCwjTshT5YLl94+EUi7Ft0fL8db3v4odiYiInBjLiJOKDPHC87c2L4j26tdH8cvJcyInIiIiZ8Uy4sTuGaXFnSN7wSIAsz/ai/Jqo9iRiIjICbGMODGJRIIXbhuCCI0HyquNmLN+L8ycP0JERN2MZcTJuSnkeHPKSLgpZNj56zm8nnlc7EhERORkWEYI/QM9sej2oQCA1785jp+OV4iciIiInAnLCAEAbh/RC/fGaiEIwNwNe1FmaBA7EhEROQmWEbJ69pYhGBTkiYoaE2Z/tBdNZovYkYiIyAmwjJCVykWGN6eMhLtChl2nKrH0f5w/QkREXY9lhFoID/BA+p3DAQDLvj2Bn09w/ggREXUtlhG6yC1RIbgvrjcA4O8f70NVHZ9fQ0REXYdlhFo1/+bBCPd3h87QgKc2H4AgcP0RIiLqGiwj1Co3hRyv3TsCcqkEXx7Q4b+5xWJHIiKiHqpDZWT58uUICwuDSqVCfHw8srOzLzm+qqoKs2bNQnBwMJRKJSIiIvDll192KDB1n2G91Hj8uggAwMLPDqLwXJ3IiYiIqCeyuYxs2LABqampWLhwIXJzcxEVFYWJEyeirKys1fEmkwnXXXcdCgoK8Mknn+Do0aPIyMhAaGjoFYenrvfI+H6IC/NFrcmMuRt4uy8REXU+iWDjZID4+HjExsZi2bJlAACLxQKtVovZs2dj3rx5F41fsWIFXn75ZRw5cgQuLi4dCmkwGKBWq6HX6+Hl5dWh16COO3O+Djcu/RHVxiY8nhiBOYkDxI5EREQOoL2f3zZ9M2IymZCTk4PExMQLLyCVIjExEVlZWa0e8/nnnyMhIQGzZs2CRqPB0KFD8eKLL8JsNrf5PkajEQaDocVG4unl44YXbruwXHxu4XmRExERUU9iUxmpqKiA2WyGRqNpsV+j0UCn07V6zMmTJ/HJJ5/AbDbjyy+/xPz58/Hqq6/iX//6V5vvk56eDrVabd20Wq0tMakL3DYiFLdEhcBsETBn/V5UNzSKHYmIiHqILr+bxmKxIDAwEO+88w5iYmKQlJSEp59+GitWrGjzmLS0NOj1eutWVFTU1TGpHf51+1D08nFFUWU9Fnx2SOw4RETUQ9hURvz9/SGTyVBaWtpif2lpKYKCglo9Jjg4GBEREZDJZNZ9gwcPhk6ng8nU+mJaSqUSXl5eLTYSn5fKBa/dGw2ZVILNe4uxee8ZsSMREVEPYFMZUSgUiImJQWZmpnWfxWJBZmYmEhISWj1m7NixOHHiBCyWC3dhHDt2DMHBwVAoFB2MTWKJ6eOLOdc2T2Cd/+khnD5XK3IiIiJydDZfpklNTUVGRgbWrFmD/Px8zJw5E7W1tUhJSQEAJCcnIy0tzTp+5syZqKysxJw5c3Ds2DFs3boVL774ImbNmtV5vwV1q1nX9EdcmC9qjE2Ysz4Pjbzdl4iIroDc1gOSkpJQXl6OBQsWQKfTITo6Gtu2bbNOai0sLIRUeqHjaLVabN++HY8//jiGDx+O0NBQzJkzB08++WTn/RbUrWRSCf5zbzRuXPoD8oqq8Nr/juMfEweKHYuIiByUzeuMiIHrjNinLw+U4NEPcyGRAB/NGI3R4X5iRyIiIjvSJeuMEP3RTcOCkTRKC0EAUjfkQV/H232JiMh2LCN0RRZMjkSYnxvO6hvw9Kd8ui8REdmOZYSuiLvywtN9t+wvwea9fLovERHZhmWErliU1tv6dN8Fnx3i032JiMgmLCPUKX5/um+NsYlP9yUiIpuwjFCnkEklWJIUBU+VHLmFVVj27QmxIxERkYNgGaFO08vHDf/67em+b3xzgk/3JSKidmEZoU51a3Qobh8RCrNFwN8/3oc6U5PYkYiIyM6xjFCne/aWIQhWq3Cqohb/3nZU7DhERGTnWEao06ldXfDSncMBAKt3FuDnExUiJyIiInvGMkJd4uqIAEwd3RsA8MTGfTA0cHVWIiJqHcsIdZm0Gwejz2+rsz7/xWGx4xARkZ1iGaEu466U45W7oyCRAJ/knMGOw6ViRyIiIjvEMkJdKjbMFw+PCwcApG06gMpak8iJiIjI3rCMUJd7/LoIRGg8UFFjxDN8mB4REf0Jywh1OZWLDEvuiYZcKsGXB3T4Yn+J2JGIiMiOsIxQtxgaqsasa/oDABZ8dhBl1Q0iJyIiInvBMkLd5rH/648hIV6oqmtE2n95uYaIiJqxjFC3cZFJseSeaChkUmQeKcMnOWfEjkRERHaAZYS61cAgTzx+XQQA4PkvDuNsVb3IiYiISGwsI9TtHr46HCN6e6Pa2IQn/7ufl2uIiJwcywh1O5lUglfvjoLKRYofj1dgXXah2JGIiEhELCMkivAAD/xz4iAAQPqXR1DMyzVERE6LZYRE85cxYRjVxwc1xiY8tYl31xAROSuWERKNVCrBS3cNh0IuxffHyrEpt1jsSEREJAKWERJVvwAPPJ7YfHfNc18cQpmBi6ERETkblhES3YxxfTEsVA1DQxPmf3aQl2uIiJwMywiJTi6T4uW7h8NFJsH2Q6X48oBO7EhERNSNWEbILgwK8mrx7JrKWpPIiYiIqLuwjJDdeHRCfwwK8sS5WhP+tfWw2HGIiKibsIyQ3VDIpXjpzuGQSIBNucXIPlUpdiQiIuoGHSojy5cvR1hYGFQqFeLj45Gdnd3m2NWrV0MikbTYVCpVhwNTzxal9cZ9cb0BNF+uaTJbRE5ERERdzeYysmHDBqSmpmLhwoXIzc1FVFQUJk6ciLKysjaP8fLyQklJiXU7ffr0FYWmnu2J6wfCx80FR3TVeD+L/1shIurpbC4jS5YswYwZM5CSkoLIyEisWLECbm5uWLVqVZvHSCQSBAUFWTeNRnNFoaln83FX4J83NC8Vv2THMa49QkTUw9lURkwmE3JycpCYmHjhBaRSJCYmIisrq83jampq0KdPH2i1Wtx66604dOjQJd/HaDTCYDC02Mi5JI3SIkrrjRpjE178Ml/sOERE1IVsKiMVFRUwm80XfbOh0Wig07W+NsTAgQOxatUqfPbZZ1i7di0sFgvGjBmDM2fOtPk+6enpUKvV1k2r1doSk3oAqVSCF24dAokE+DTvLH45eU7sSERE1EW6/G6ahIQEJCcnIzo6GuPHj8emTZsQEBCAt99+u81j0tLSoNfrrVtRUVFXxyQ7NLyXN+7/w2TWRk5mJSLqkWwqI/7+/pDJZCgtLW2xv7S0FEFBQe16DRcXF4wYMQInTpxoc4xSqYSXl1eLjZzTExObJ7MeK63Bez+fEjsOERF1AZvKiEKhQExMDDIzM637LBYLMjMzkZCQ0K7XMJvNOHDgAIKDg21LSk7J202BeTdemMx6qqJW5ERERNTZbL5Mk5qaioyMDKxZswb5+fmYOXMmamtrkZKSAgBITk5GWlqadfzzzz+Pr7/+GidPnkRubi6mTp2K06dP46GHHuq834J6tHtGaTG2vx8aGi14YuM+mC18kB4RUU8it/WApKQklJeXY8GCBdDpdIiOjsa2bdusk1oLCwshlV7oOOfPn8eMGTOg0+ng4+ODmJgY7Ny5E5GRkZ33W1CPJpFI8NKdwzHxPz9gz+nzeO/nU3hoXLjYsYiIqJNIBAd4XrvBYIBarYZer+f8ESf24a7TeHrzQSjlUnw1ZxzCAzzEjkRERJfQ3s9vPpuGHMb9cb1xVX9/GJsseOKT/bxcQ0TUQ7CMkMOQSCRYfOcweCjlyPntcg0RETk+lhFyKL183PD0pMEAgJe3H8Wv5TUiJyIioivFMkIO595YLcYNaL5ck/bfA3CAaU9ERHQJLCPkcJov1wyHq4sM2QWV+HzfWbEjERHRFWAZIYcU6u2KWdf0AwCkf3kEtcYmkRMREVFHsYyQw3poXDi0vq7QGRqw/Nu2Hy9ARET2jWWEHJbKRYb5k5oXz3v3x1Mo4FLxREQOiWWEHNp1kRpcHREAk9mCF7YcFjsOERF1AMsIOTSJRIIFN0dCLpUg80gZvj1SJnYkIiKyEcsIObz+gR5IGRsGAHh+y2GYmiziBiIiIpuwjFCP8LdrB8DfQ4lTFbVYxZVZiYgcCssI9QieKhfMu3EQAOC1/x1HUWWdyImIiKi9WEaox7hjRCji+/qivtGMpzZzZVYiIkfBMkI9hlQqQfodw6CQS/Hj8Qps3lssdiQiImoHlhHqUcIDPDA3cQCA5smsFTVGkRMREdHlsIxQjzNjXDgGB3uhqq4Rz3/BtUeIiOwdywj1OC4yKV66cxikEuDzfWfxzZFSsSMREdElsIxQjzS8lzceGhcOAHh680HU8EF6RER2i2WEeqzHEyPQ29cNJfoG/HvbEbHjEBFRG1hGqMdyVciQfscwAMAHv5xGXlGVuIGIiKhVLCPUo43t7487RoZCEICnNh1Ak5lLxRMR2RuWEerxnr5pMLzdXHC4xIDVOwvEjkNERH/CMkI9np+HEmm/LRW/ZMcxFFfVi5yIiIj+iGWEnMLdMVqM6uODOpMZz35+SOw4RET0Bywj5BSkUglevGMY5FIJdhwuxfZDOrEjERHRb1hGyGlEaDwx4+rmtUee/fwQ1x4hIrITLCPkVP72fwOg9XVFib4B/9lxTOw4REQElhFyMq4KGZ6/dSgAYPXOAhzVVYuciIiIWEbI6VwzMBA3DAmC2SJg4ecHIQiC2JGIiJxah8rI8uXLERYWBpVKhfj4eGRnZ7fruPXr10MikeC2227ryNsSdZqnJw2GUi7FLycrsfVAidhxiIicms1lZMOGDUhNTcXChQuRm5uLqKgoTJw4EWVlZZc8rqCgAP/4xz8wbty4Docl6ixaXzfMnNAPALBoaz7qTJzMSkQkFpvLyJIlSzBjxgykpKQgMjISK1asgJubG1atWtXmMWazGVOmTMFzzz2H8PDwKwpM1FkeGd8PvXyaJ7Mu//aE2HGIiJyWTWXEZDIhJycHiYmJF15AKkViYiKysrLaPO75559HYGAgHnzwwY4nJepkKhcZ5t8cCQDI+OEUCipqRU5EROScbCojFRUVMJvN0Gg0LfZrNBrodK0vIvXTTz9h5cqVyMjIaPf7GI1GGAyGFhtRV7g+UoNxA/xhMlvwwpbDYschInJKXXo3TXV1NR544AFkZGTA39+/3celp6dDrVZbN61W24UpyZlJJBIsnDwEcqkEmUfK8O2RS899IiKizmdTGfH394dMJkNpaWmL/aWlpQgKCrpo/K+//oqCggJMnjwZcrkccrkc77//Pj7//HPI5XL8+uuvrb5PWloa9Hq9dSsqKrIlJpFN+gd64MGr+gIAnvviEIxNZpETERE5F5vKiEKhQExMDDIzM637LBYLMjMzkZCQcNH4QYMG4cCBA8jLy7Nut9xyC6655hrk5eW1+Y2HUqmEl5dXi42oK82+dgACPZUoOFeH934uEDsOEZFTkdt6QGpqKqZNm4ZRo0YhLi4OS5cuRW1tLVJSUgAAycnJCA0NRXp6OlQqFYYOHdrieG9vbwC4aD+RmDyUcsy7cRBSP96HNzKP4/YRodB4qcSORUTkFGwuI0lJSSgvL8eCBQug0+kQHR2Nbdu2WSe1FhYWQirlwq7keG6LDsUHv5zG3sIqvPTVESxJihY7EhGRU5AIDrAWtsFggFqthl6v5yUb6lL7iqpw25s/QxCA/84cg5g+PmJHIiJyWO39/OZXGER/EKX1xt0xvQAAz35+CBaL3Xd1IiKHxzJC9CdPTBwET6UcB4r12JjDO7mIiLoaywjRnwR4KjEncQAA4N/bjkJf3yhyIiKino1lhKgVyQlh6BfgjnO1JryeeVzsOEREPRrLCFErFHIpFkweAgBYs7MAx0urRU5ERNRzsYwQtWF8RACui9SgySJgwWeH4AA3nhEROSSWEaJLWHBzJJRyKbJOnsMX+0vEjkNE1COxjBBdgtbXDbOu6Q8AWLT1MGqMTSInIiLqeVhGiC7j4avD0cfPDaUGIyezEhF1AZYRostQucjw7C3Nk1lX/XQKxziZlYioU7GMELXDNQMDcb11MutBTmYlIupELCNE7TT/t8msv5ysxOf7zoodh4iox2AZIWonra8bHrNOZs2HoYErsxIRdQaWESIbzLg6HH393VFWbcRznx8WOw4RUY/AMkJkA5WLDC/fNRwSCfDf3DPYfkgndiQiIofHMkJko1Fhvnj46nAAwFObDqCixihyIiIix8YyQtQBqddFYKDGE+dqTXh68wHeXUNEdAVYRog6QCmXYUlSFFxkEmw/VIrNe4vFjkRE5LBYRog6aEiIGnOuHQAAWPjZIZytqhc5ERGRY2IZIboCj4zvh2itN6qNTXjik32wWHi5hojIViwjRFdALpNiyT1RULlI8fOJc/gk94zYkYiIHA7LCNEVCg/wwOOJEQCAxV8dQVWdSeRERESOhWWEqBNMv6ovBgR6oLLWhJe3HxU7DhGRQ2EZIeoELjIpXrhtKABgXXYh9hVViRuIiMiBsIwQdZLR4X64fUQoBAGY/9lBmDmZlYioXVhGiDpR2k2D4KmUY/8ZPT7KLhQ7DhGRQ2AZIepEgZ4q/P365smsL28/inNcKp6I6LJYRog62dTRfRAZ7AV9fSMWf3VE7DhERHaPZYSok8n/MJl1Y84Z7CmoFDkREZF9Yxkh6gIxfXxwb6wWAPD05oNoNFtETkREZL9YRoi6yJM3DIKvuwJHS6ux8qdTYschIrJbHSojy5cvR1hYGFQqFeLj45Gdnd3m2E2bNmHUqFHw9vaGu7s7oqOj8cEHH3Q4MJGj8HFX4KmbBgMAlv7vGIoq60RORERkn2wuIxs2bEBqaioWLlyI3NxcREVFYeLEiSgrK2t1vK+vL55++mlkZWVh//79SElJQUpKCrZv337F4Yns3Z0jQxHf1xcNjRY8+/khCALXHiEi+jOJYOOfjvHx8YiNjcWyZcsAABaLBVqtFrNnz8a8efPa9RojR47EpEmT8MILL7RrvMFggFqthl6vh5eXly1xiUR3oqwaN772IxrNAlZMjcENQ4PEjkRE1C3a+/lt0zcjJpMJOTk5SExMvPACUikSExORlZV12eMFQUBmZiaOHj2Kq6++us1xRqMRBoOhxUbkqPoHeuKvV/cDADz3xSHUGJtETkREZF9sKiMVFRUwm83QaDQt9ms0Guh0ujaP0+v18PDwgEKhwKRJk/DGG2/guuuua3N8eno61Gq1ddNqtbbEJLI7j/1ff/T2dUOJvgH/2XFM7DhERHalW+6m8fT0RF5eHnbv3o1FixYhNTUV3333XZvj09LSoNfrrVtRUVF3xCTqMioXGZ6/dQgA4L2fT+HQWb3IiYiI7IfclsH+/v6QyWQoLS1tsb+0tBRBQW1fB5dKpejfvz8AIDo6Gvn5+UhPT8eECRNaHa9UKqFUKm2JRmT3JgwMxKThwdi6vwQLPjuETx5JgEQiETsWEZHobPpmRKFQICYmBpmZmdZ9FosFmZmZSEhIaPfrWCwWGI18Zgc5n2cmDYabQoac0+exeW+x2HGIiOyCzZdpUlNTkZGRgTVr1iA/Px8zZ85EbW0tUlJSAADJyclIS0uzjk9PT8eOHTtw8uRJ5Ofn49VXX8UHH3yAqVOndt5vQeQggtWumP1/AwAAL355BIaGRpETERGJz6bLNACQlJSE8vJyLFiwADqdDtHR0di2bZt1UmthYSGk0gsdp7a2Fo8++ijOnDkDV1dXDBo0CGvXrkVSUlLn/RZEDmT6VWHYuKcIJytq8dr/jmP+zZFiRyIiEpXN64yIgeuMUE/z/bFyTFuVDZlUgq/mjEOExlPsSEREna5L1hkhos4xPiIAE4doYLYIWPgZV2YlIufGMkIkkmcmRUIplyLr5Dls2V8idhwiItGwjBCJROvrhkcnNN/yvmhrPmq5MisROSmWESIR/XV8OLS+rtAZGjBv0wFeriEip8QyQiQilYsML98VBblUgi/2ncXS/x0XOxIRUbdjGSES2ehwPyy6fSgA4LXM4/gsj4uhEZFzYRkhsgNJsb3x16vDAQBPbNyPnNOVIiciIuo+LCNEduLJGwbh+kgNTGYLHn4/B0WVdWJHIiLqFiwjRHZCKpVg6b3RGBrqhXO1JkxfvZvLxRORU2AZIbIjbgo53k2OhcZLieNlNXj2s0NiRyIi6nIsI0R2JkitwltTYyCRAJv2FuP7Y+ViRyIi6lIsI0R2aGRvH/xlTBgA4KlNB7ggGhH1aCwjRHbqH9cPRKi3K4qr6rFkxzGx4xARdRmWESI75a6UW9cfee/nU8grqhI3EBFRF2EZIbJjEwYG4vYRobAIwLz/7oepySJ2JCKiTscyQmTn5t8cCV93BY7oqvHOD7+KHYeIqNOxjBDZOV93BRbcHAkAeD3zBE6U1YiciIioc7GMEDmAW6NDMGFgAExmCx5bl4t6k1nsSEREnYZlhMgBSCQSLL5jOPw9mi/XzNu0H4IgiB2LiKhTsIwQOYggtQrL7x8JmVSCz/LOYvXOArEjERF1CpYRIgcSH+6Hp28aDABYtDUfu06eEzkREdGVYxkhcjApY8Nwa3QImiwCZq3bC52+QexIRERXhGWEyMFIJBKk3zEMg4I8UVFjxMwPc2Bs4oRWInJcLCNEDshNIcfbD8TASyXH3sIqPPv5YU5oJSKHxTJC5KD6+LnjtftGQCIBPsouxPtZp8WORETUISwjRA7smoGBmHfDIADA81sO46fjFSInIiKyHcsIkYN7+Opw3DEyFGaLgEc/zMGpilqxIxER2YRlhMjBSSQSvHj7MIzs7Q1DQxMeXLMb+vpGsWMREbUbywhRD6BykWHFAzEIUatwsrwWsz/aiyYzn/BLRI6BZYSohwj0VOGd5FFwdZHhh2PleHn7UbEjERG1S4fKyPLlyxEWFgaVSoX4+HhkZ2e3OTYjIwPjxo2Dj48PfHx8kJiYeMnxRNRxQ0PVePWeKADA2z+cxLdHy0RORER0eTaXkQ0bNiA1NRULFy5Ebm4uoqKiMHHiRJSVtf6H3nfffYf77rsP3377LbKysqDVanH99dejuLj4isMT0cVuGhaMaQl9AAD/+HgfygxcoZWI7JtEsHGlpPj4eMTGxmLZsmUAAIvFAq1Wi9mzZ2PevHmXPd5sNsPHxwfLli1DcnJyu97TYDBArVZDr9fDy8vLlrhETqmh0Yzblv+MI7pqXNXfH+9Pj4NUKhE7FhE5mfZ+ftv0zYjJZEJOTg4SExMvvIBUisTERGRlZbXrNerq6tDY2AhfX19b3pqIbKBykWHZ/SPg6iLDTycq8PYPJ8WORETUJpvKSEVFBcxmMzQaTYv9Go0GOp2uXa/x5JNPIiQkpEWh+TOj0QiDwdBiIyLb9A/0xLO3RAIAXv36KPYWnhc5ERFR67r1bprFixdj/fr12Lx5M1QqVZvj0tPToVarrZtWq+3GlEQ9xz2jtJg0PBhNFgF/W78XhgauP0JE9semMuLv7w+ZTIbS0tIW+0tLSxEUFHTJY1955RUsXrwYX3/9NYYPH37JsWlpadDr9datqKjIlphE9JvfF0Tr5eOKosp6/O2jvWho5BN+ici+2FRGFAoFYmJikJmZad1nsViQmZmJhISENo/797//jRdeeAHbtm3DqFGjLvs+SqUSXl5eLTYi6hi1qwtev28EFHIpvjtajgdW7oK+jt+QEJH9sPkyTWpqKjIyMrBmzRrk5+dj5syZqK2tRUpKCgAgOTkZaWlp1vEvvfQS5s+fj1WrViEsLAw6nQ46nQ41NTWd91sQ0SWN7O2DD6bHwVMlx+6C87jn7Szo9Lzll4jsg81lJCkpCa+88goWLFiA6Oho5OXlYdu2bdZJrYWFhSgpKbGOf+utt2AymXDXXXchODjYur3yyiud91sQ0WXFh/vh478mINBTiaOl1bjzrZ34tZx/KSAi8dm8zogYuM4IUecpqqzDtFXZOFlRC193Bd77SyyitN5ixyKiHqhL1hkhIsen9XXDxkcSMLyXGpW1Jkx9dxfyiqrEjkVEToxlhMgJ+Xko8dGM0Yjv64tqYxMeWLkL+89UiR2LiJwUywiRk3JXyrHqL7GIC/NFdUMTpr67CweL9WLHIiInxDJC5MTclXKsSonFqD4+MDQ0YQoLCRGJgGWEyMl5KOV4LyUWI3t7Q1/fiKkrd+HwWT6CgYi6D8sIEcFT5YLV0+MQpfVGVV0jkt7OwrdHysSORUROgmWEiAAAXioXvD89DrFhPqg2NmH6mt1Y/u0JOMDd/0Tk4FhGiMhK7eqCDx8ajfvje0MQgJe3H8Vj6/aiztQkdjQi6sFYRoioBYVcihdvH4ZFtw+Fi0yCrQdKcMebO1FUWSd2NCLqoVhGiKhVU+L7YN2M0fD3UOKIrhq3v7kTp8/Vih2LiHoglhEialNsmC++mD0Wg4I8UVFjxNSVu1Bq4AP2iKhzsYwQ0SUFq13x/oNx6OPnhqLKeiSvzEZVnUnsWETUg7CMENFlBXqqsPbBeGi8mp/4O331bk5qJaJOwzJCRO2i9XXD+9PjoXZ1QW5hFR5ZmwtTk0XsWETUA7CMEFG7DQzyxKq/xMLVRYYfjpVj7oa9MDaZxY5FRA6OZYSIbBLTxwdvPxADF5kEXx7Q4f6MXaioMYodi4gcGMsIEdns6ogArPpLLDxVcuScPo9bl/2M/BI+z4aIOoZlhIg6ZNyAAHw6ayz6+rujuKoed761E18f0okdi4gcEMsIEXVYvwAPfProWFzV3x91JjP+ujYHr2ce58RWIrIJywgRXRG1mwveS4lFckIfCAKwZMcxXPef77F1fwkfskdE7cIyQkRXzEUmxfO3DsWrd0fB30OJ0+fqMGtdLm5/cyeyT1WKHY+I7JxEcIC/uhgMBqjVauj1enh5eYkdh4guodbYhIwfT+KdH06iztR82++NQ4Pw77uGw1PlInI6IupO7f385jcjRNSp3JVyzE2MwHf/mID74npDKgG+Oth8C/A53gJMRK1gGSGiLhHopUL6HcOw+dGx8HVX4ECxHnevyEJxVb3Y0YjIzrCMEFGXitJ6Y+MjCQj1dsXJilrc9dZOnCirFjsWEdkRlhEi6nL9Ajyw8ZEE9AtwR4m+AXevyEJeUZXYsYjITrCMEFG3CPF2xcZHxiCqlxrn6xpxz9tZ+PvH+5BzupK3ABM5Od5NQ0TdqsbYhFkf5uL7Y+XWfREaD9wX1xu3jwiFt5tCxHRE1Jna+/nNMkJE3U4QBOwtqsJHuwrxxf6zaGhsXrHV1UWGx68bgOlj+0Iu4xe3RI6OZYSIHIKhoRGf7S3Gh7sKcUTXPLF1SIgX0u8YhuG9vMUNR0RXhGWEiByKIAjYmHMGi7bmQ1/fCKkESBnbF6nXRcBdKRc7HhF1QJcuerZ8+XKEhYVBpVIhPj4e2dnZbY49dOgQ7rzzToSFhUEikWDp0qUdeUsi6uEkEgnuGaVF5t/H49boEFgEYOVPp3D9f37Alv1nOcmVqAezuYxs2LABqampWLhwIXJzcxEVFYWJEyeirKys1fF1dXUIDw/H4sWLERQUdMWBiahn8/dQ4rV7R2B1Six6+biiuKoej63bizvf2omc0+fFjkdEXcDmyzTx8fGIjY3FsmXLAAAWiwVarRazZ8/GvHnzLnlsWFgY5s6di7lz59oUkpdpiJxTnakJGT+cworvf0V9Y/NzbiYND8a8GwZB6+smcjoiupz2fn7bdCHWZDIhJycHaWlp1n1SqRSJiYnIysrqeNo/MRqNMBovPMPCYDB02msTkeNwU8gxJ3EA7ovT4tWvj+HjnCJs3V+C7Qd1GBqqRmyYD0aF+WJUHx/4eSjFjktEHWRTGamoqIDZbIZGo2mxX6PR4MiRI50WKj09Hc8991ynvR4RObZALxVeums4po0Jw4tf5uOnExXIK6pCXlEVMn48BQDoH+iBaWPCcG+sFi68LZjIodjl/2PT0tKg1+utW1FRkdiRiMgORIZ4Ye1D8fjxn9fgP0lRuD++NyI0HgCAE2U1mP/pQSQu+R6f7zsLi4UTXokchU3fjPj7+0Mmk6G0tLTF/tLS0k6dnKpUKqFU8itXImqd1tcNWl833D6iFwCgqs6Ez/LO4o1vjuP0uTr87aO9WPHdr3jihoGYEBEAiUQicmIiuhSbvhlRKBSIiYlBZmamdZ/FYkFmZiYSEhI6PRwRUXt4uykwbUwYvn/iGvzj+gh4KuU4XGJAynu7Mfujvag1NokdkYguwebLNKmpqcjIyMCaNWuQn5+PmTNnora2FikpKQCA5OTkFhNcTSYT8vLykJeXB5PJhOLiYuTl5eHEiROd91sQEQFwV8rx2P8NwA//vAYzxvWFXCrBlv0luHX5zzhRViN2PCJqQ4dWYF22bBlefvll6HQ6REdH4/XXX0d8fDwAYMKECQgLC8Pq1asBAAUFBejbt+9FrzF+/Hh899137Xo/3tpLRB2xp6ASs9blotRghLtChpfuGo6bh4eIHYvIaXA5eCIiAOXVRsz+KBe/nKwEAEwf2xfzbhwEhdwu5+8T9Shduhw8EZGjCPBUYu2D8XhkfD8AwKqfT+Gql77B0v8dQ5mhQeR0RATwmxEiciLbD+nwzKcHUV7dvKiiXCrBjcOCMS2hD2L6+PCuG6JOxss0REStMDVZsO2QDu/vLMCePzzrJkStwrgBAbg6IgBj+/vB200hYkqinoFlhIjoMg4W6/FB1ml8tq8YDY0W636JBBjeyxsPXdUXk6M44ZWoo1hGiIjaqd5kxq5T5/Dj8Qr8eLwcx0ov3AacnNAHz0yK5IRXog5gGSEi6iCdvgHvZxXgze9+BQCM6O2NN6eMRLDa9aKx1Q2NcHWRQc7n4RBdhGWEiOgKfXOkFHPX58HQ0AQ/dwXeuG8EYsJ8kFNwHt8fL8ePxypwuMSAAE8lZo7vh/vje0PlIhM7NpHdYBkhIuoEhefq8MjaHBwuMUAqAZRyGeobza2ODfRUYtY1/ZEUq2UpIQLLCBFRp2loNGP+pwexMecMgOa1S8YN8MfVAwIwOtwP3x4tw7JvTqC4qh4AEKxW4cGr+uLm4SEIUqvEjE4kKpYRIqJOJAgCDpcYIIEEg4M9L1qTxNhkxsY9Z7D82xMo0TcvpiaRALFhvpgcFYIbhwbB34NPIyfnwjJCRCQCY5MZn+Scwad7i7G74MI6JlIJ0NvXDRYBMFsEmC0CmiwCBgV54t93DUeI98WTY4kcHcsIEZHIzlbVY+v+EmzZfxb7zujbHBfgqURG8ihEa727LxxRN2AZISKyI0WVddAZGiCTSiCTSCCTSmBssuDpzQdwRFcNpVyKV+6O4iJr1KOwjBAROYAaYxPmfLQXmUfKAABzEwdgzrUDIJFIoK9rxPGyahwrrYG+vhGBnkpovFQIUisR6KWCp1LO5+mQXWMZISJyEGaLgMVf5SPjx1MAgEFBnjhXa7I+0K8tQV4qvHTXcIyPCOiOmEQ2YxkhInIwH2UXYv6nB9FkufDHcohahQEaT/i5K1BWbUSpoQGlhgYYGpoANE+MfWLiIDwyPpzfkpDdYRkhInJAx0qrcbBYj77+7ugf6AFPlUur42qMTfjXlsNYv7sIADBpeDBevms43BTyi8YKgoBztSacPleHoso6nD5Xh/pGM+6L06KPn3uX/j7k3FhGiIh6OEEQ8OGuQjz7+SHrbcJvTY2BqcmCA8V6HCzW40CxHkdKDKg1XbxqrKdKjiX3ROO6SI0I6ckZsIwQETmJ3QWVmLk2FxU1bc8xkUiAYC8VtL5u6OPnhqOlNdhXVAUAmDmhH/5+XQQf9kedjmWEiMiJlOjrMXNtLvKKquCukGFIiBrDeqkxLFSNISFe6O3nBqX8wvNyTE0WpH+Vj/d+LgAAJIT74fX7RiDAk6vEUudhGSEicjKCIKCs2gh/DyVk0vZNZt2y/yz++cl+1JnM0HgpcVt0KCI0nhgY5In+gR584B9dEZYRIiJqlxNl1XhkbS5OlNW02C+RAGF+7hih9UZ8uC/i+vohzM/NetfO2ap6/HyiAj+fqMDugvNQyqUI9lYhWO2KYHXzP8cPDEAol7p3WiwjRETUbrXGJmzZfxaHzxpwtLQaR3XVOF/XeNG4QE8lorTe+LWsBicrai/7um4KGebfHIl7Y7W89dgJsYwQEVGHCYKAihoTDp3VY3dBJbJPVWJfkR4ms8U6RioBorTeuKq/PxLC/SCRSFCir0eJvgFnq+qx70wVDhYbAACJgzVYfOewdj25+PfLTRIJEOip6rLfkboeywgREXWqhkYz9hZW4WCxHn383BAf7ge1a+vroADNK8uu/OkkXtl+DCazBX7uCrx053Ak/nYrsbHJDH1dI87XNeJEWQ0OndXj4FkDDp/Vo6LGBKD5m5hhoWoMCW2ejDuytzf8LlNoBEHo8LcwpiYLjuqqMSTEC9J2zruhtrGMEBGRXcgvMeDxDXk4oqsGAASrVdDXN6KulbVPfvd7D7D86RNKJpVg3AB/3DGyF66P1Fgn2AqCgAPFenyx7yy27C9BrbEJD14VjhlX9211IbjWNDSakbwqG9mnKhGh8cDcxAjcMCSIpeQKsIwQEZHdaGg0Y8mOY8j48ST++KkjlQBqVxf08nHD0FAvDAlpvhV5UJAXBAg4fNbw2+JtBhworsKx0guTbD2Vctw0LBgBnkps2X8WBefqLnrfQE8lUq+LwF0xvS65jorZImD2R7n48oCuxf5BQZ6YmxiBiUM0nPPSASwjRERkdwrP1aGyzgRvVxf4uCngqZLb9M3DyfIabN5bjE25xSiuqm/xM5WLFImDNZgcFQJTkwUvbz+KwsrmghKh8cC8GwfhmoGBF5UKQRDw3BeHsXpnAVxkEiy7fyQOnzVg1U+nUG1sfgZQZLAX7hnVC9cPCUII7w5qN5YRIiLqsSwWAdkFlfh0bzGqjU24PlKDxMEauCsvXJIxNpmx9pdCvPHNcVT9dmdQXJgv/nnDQIwK87WOW/H9r1j81REAwOv3jcAtUSEAgKo6E9798RTe+/lUi+X0o7TemDhEg+sjgxDu737JMtXQaMaZ8/VoaDTDIggwW5o3iwC4usjg5SqHp8oFnio5XHrgCrgsI0RERAD09Y1489sTWL2zAMam5ruBrh0UiH9MHIgjOgMe37APAPDMpMF4aFz4RcdX1prw35wz2H5Ih5zC8y0uMynkUvTydkUvXzdofVwR5KVCaXUDTlXUoqCiDmf19Wjvp6ybQoYATyVCvV2bN5/mfw4NVWNQkKdDXibq0jKyfPlyvPzyy9DpdIiKisIbb7yBuLi4Nsdv3LgR8+fPR0FBAQYMGICXXnoJN910U7vfj2WEiIiuVIm+Hq9nHsfHe87AbBEgkQBSiQRmi4CHruqLZ26OvOxrlFU3YMfhUmw/VIqsXyvQaL78R6iHUg53pQwyiQRSqQQyqQRSiQT1JjMMDZeeyPu7IC8VJgwMwISBgRjb3w8eSjkqakworKxFYWUdCs/VwyIICPBUItBTiUAvFQI9lVC5yFBnakKdyYxaY/M/XWRShPm5IcBT2eUFp8vKyIYNG5CcnIwVK1YgPj4eS5cuxcaNG3H06FEEBgZeNH7nzp24+uqrkZ6ejptvvhnr1q3DSy+9hNzcXAwdOrRTfxkiIqLLOVleg1d3HMPW/SUAgMlRIXgtKdrmu2aazBaU6BtQVFmHovN1KKqsh87QgEBPJfr6u6OvvzvC/N3h56645Id+o9mCmoYm6OsboTM0oPh8Pc5W1aO4qh6FlXXILTyPhsYL67u4yCSQS6Wob7x8ibkUVxcZ+vi5oa+/O/r4uSMpVou+/u5X9Jp/1mVlJD4+HrGxsVi2bBkAwGKxQKvVYvbs2Zg3b95F45OSklBbW4stW7ZY940ePRrR0dFYsWJFp/4yRERE7XWwWI/DJQbcFh0Khdx+52s0NJqx61Qlvj1Shu+PlePUbyvfSiRAiNoVWl9X9PZ1g1wmRZnBiPLqBpRXG1FeY0SjWYDKRQo3hRxuChncFXLUNTah+Hz9RbdN/3dmAmL6+LaSoOPa+/ndvpuvf2MymZCTk4O0tDTrPqlUisTERGRlZbV6TFZWFlJTU1vsmzhxIj799FNb3pqIiKhTDQ1VY2ioWuwYl6VykWF8RADGRwQAAIoq69BotqCXj9slS5TFIkAAWn1ooqnJgjPn63D6XB1OVdTi9Lla9Avw6Kpf4bJsKiMVFRUwm83QaDQt9ms0Ghw5cqTVY3Q6XavjdTpdq+MBwGg0wmg0Wv+7wWCwJSYREVGPpfV1a9e4S112UsilCA/wQHiAB67prGBXwC6/l0pPT4darbZuWq1W7EhERETURWwqI/7+/pDJZCgtLW2xv7S0FEFBQa0eExQUZNN4AEhLS4Ner7duRUVFtsQkIiIiB2JTGVEoFIiJiUFmZqZ1n8ViQWZmJhISElo9JiEhocV4ANixY0eb4wFAqVTCy8urxUZEREQ9k01zRgAgNTUV06ZNw6hRoxAXF4elS5eitrYWKSkpAIDk5GSEhoYiPT0dADBnzhyMHz8er776KiZNmoT169djz549eOeddzr3NyEiIiKHZHMZSUpKQnl5ORYsWACdTofo6Ghs27bNOkm1sLAQUumFL1zGjBmDdevW4ZlnnsFTTz2FAQMG4NNPP233GiNERETUs3E5eCIiIuoS7f38tsu7aYiIiMh5sIwQERGRqFhGiIiISFQsI0RERCQqlhEiIiISFcsIERERiYplhIiIiERl86JnYvh9KRQ+vZeIiMhx/P65fbklzRyijFRXVwMAn95LRETkgKqrq6FWq9v8uUOswGqxWHD27Fl4enpCIpF02usaDAZotVoUFRVxZVc7wvNif3hO7A/PiX3ieWlJEARUV1cjJCSkxaNi/swhvhmRSqXo1atXl70+nwxsn3he7A/Pif3hObFPPC8XXOobkd9xAisRERGJimWEiIiIROXUZUSpVGLhwoVQKpViR6E/4HmxPzwn9ofnxD7xvHSMQ0xgJSIiop7Lqb8ZISIiIvGxjBAREZGoWEaIiIhIVCwjREREJCqnLiPLly9HWFgYVCoV4uPjkZ2dLXYkp5Geno7Y2Fh4enoiMDAQt912G44ePdpiTENDA2bNmgU/Pz94eHjgzjvvRGlpqUiJnc/ixYshkUgwd+5c6z6ek+5XXFyMqVOnws/PD66urhg2bBj27Nlj/bkgCFiwYAGCg4Ph6uqKxMREHD9+XMTEPZ/ZbMb8+fPRt29fuLq6ol+/fnjhhRdaPH+F58VGgpNav369oFAohFWrVgmHDh0SZsyYIXh7ewulpaViR3MKEydOFN577z3h4MGDQl5ennDTTTcJvXv3FmpqaqxjHnnkEUGr1QqZmZnCnj17hNGjRwtjxowRMbXzyM7OFsLCwoThw4cLc+bMse7nOelelZWVQp8+fYS//OUvwq5du4STJ08K27dvF06cOGEds3jxYkGtVguffvqpsG/fPuGWW24R+vbtK9TX14uYvGdbtGiR4OfnJ2zZskU4deqUsHHjRsHDw0N47bXXrGN4XmzjtGUkLi5OmDVrlvW/m81mISQkREhPTxcxlfMqKysTAAjff/+9IAiCUFVVJbi4uAgbN260jsnPzxcACFlZWWLFdArV1dXCgAEDhB07dgjjx4+3lhGek+735JNPCldddVWbP7dYLEJQUJDw8ssvW/dVVVUJSqVS+Oijj7ojolOaNGmSMH369Bb77rjjDmHKlCmCIPC8dIRTXqYxmUzIyclBYmKidZ9UKkViYiKysrJETOa89Ho9AMDX1xcAkJOTg8bGxhbnaNCgQejduzfPURebNWsWJk2a1OLfPcBzIobPP/8co0aNwt13343AwECMGDECGRkZ1p+fOnUKOp2uxTlRq9WIj4/nOelCY8aMQWZmJo4dOwYA2LdvH3766SfceOONAHheOsIhHpTX2SoqKmA2m6HRaFrs12g0OHLkiEipnJfFYsHcuXMxduxYDB06FACg0+mgUCjg7e3dYqxGo4FOpxMhpXNYv349cnNzsXv37ot+xnPS/U6ePIm33noLqampeOqpp7B792787W9/g0KhwLRp06z/3lv7s4znpOvMmzcPBoMBgwYNgkwmg9lsxqJFizBlyhQA4HnpAKcsI2RfZs2ahYMHD+Knn34SO4pTKyoqwpw5c7Bjxw6oVCqx4xCai/qoUaPw4osvAgBGjBiBgwcPYsWKFZg2bZrI6ZzXxx9/jA8//BDr1q3DkCFDkJeXh7lz5yIkJITnpYOc8jKNv78/ZDLZRXcBlJaWIigoSKRUzumxxx7Dli1b8O2336JXr17W/UFBQTCZTKiqqmoxnueo6+Tk5KCsrAwjR46EXC6HXC7H999/j9dffx1yuRwajYbnpJsFBwcjMjKyxb7BgwejsLAQAKz/3vlnWfd64oknMG/ePNx7770YNmwYHnjgATz++ONIT08HwPPSEU5ZRhQKBWJiYpCZmWndZ7FYkJmZiYSEBBGTOQ9BEPDYY49h8+bN+Oabb9C3b98WP4+JiYGLi0uLc3T06FEUFhbyHHWRa6+9FgcOHEBeXp51GzVqFKZMmWL9zzwn3Wvs2LEX3fJ+7Ngx9OnTBwDQt29fBAUFtTgnBoMBu3bt4jnpQnV1dZBKW358ymQyWCwWADwvHSL2DFqxrF+/XlAqlcLq1auFw4cPCw8//LDg7e0t6HQ6saM5hZkzZwpqtVr47rvvhJKSEutWV1dnHfPII48IvXv3Fr755hthz549QkJCgpCQkCBiaufzx7tpBIHnpLtlZ2cLcrlcWLRokXD8+HHhww8/FNzc3IS1a9daxyxevFjw9vYWPvvsM2H//v3CrbfeyltIu9i0adOE0NBQ6629mzZtEvz9/YV//vOf1jE8L7Zx2jIiCILwxhtvCL179xYUCoUQFxcn/PLLL2JHchoAWt3ee+8965j6+nrh0UcfFXx8fAQ3Nzfh9ttvF0pKSsQL7YT+XEZ4TrrfF198IQwdOlRQKpXCoEGDhHfeeafFzy0WizB//nxBo9EISqVSuPbaa4WjR4+KlNY5GAwGYc6cOULv3r0FlUolhIeHC08//bRgNBqtY3hebCMRhD8sGUdERETUzZxyzggRERHZD5YRIiIiEhXLCBEREYmKZYSIiIhExTJCREREomIZISIiIlGxjBAREZGoWEaIiIhIVCwjREREJCqWESIiIhIVywgRERGJimWEiIiIRPX/CdZJvxlQw/UAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "avg_loss = torch.tensor(losses).view(-1, 100).mean(1)\n",
    "plt.plot(avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "id": "607b996d-13c3-4612-a14e-b524a3183377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|starttext|>', 'b', 'æ̰ː', 'ǀ', 't̚', '<|endoftext|>']\n"
     ]
    }
   ],
   "source": [
    "print(decode(model1.generate(idx, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c783e931-d076-439a-b47f-1b99fa7039e9",
   "metadata": {},
   "source": [
    "## Weighted Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5061af6c-fbbc-424b-bdbe-c7dc2e8ce0c9",
   "metadata": {},
   "source": [
    "First let's just use the mean/average of the first `t` tokens to predict the `t+1` token. What is important is that the `t+1` token only uses the previous tokens to take an average. It should not use any future tokens since we are proceeding strictly left to right in the time dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "id": "c1662862-0d0f-4b24-a0ca-26bdf343d656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "block_size = max_len-2\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "id": "41f0ac1a-4f11-42be-b548-db129f4d9330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 41, 312])"
      ]
     },
     "execution_count": 717,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(batch_size, block_size, vocab_size)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "id": "71aea7e3-4012-4417-8b6b-86f5aeb76069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0858,  0.5179, -0.5062,  ..., -2.2934,  3.0562, -0.4245],\n",
       "        [-0.2002, -1.8072,  1.2379,  ..., -0.2050,  0.4118,  0.1319],\n",
       "        [-0.9770,  0.3938,  1.4091,  ...,  0.7232,  1.1109,  1.2289],\n",
       "        ...,\n",
       "        [ 0.2421,  0.1810,  0.4489,  ..., -1.9890,  0.3847,  0.0953],\n",
       "        [-0.5978, -0.4507,  0.5413,  ..., -0.6299,  0.4070,  1.6370],\n",
       "        [-0.3983, -0.6930,  0.5295,  ...,  0.8024,  2.5352, -0.2173]])"
      ]
     },
     "execution_count": 720,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330938b1-6a56-4b0c-885e-d0c5f6e4d25a",
   "metadata": {},
   "source": [
    "### Weighted Average: Approach 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "id": "26290da6-292a-47c5-8fed-6d8ff49117be",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mean = torch.zeros((batch_size, block_size, vocab_size))\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        x_mean[b, t] = torch.mean(x[b,:t+1], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "id": "98e08e46-f960-427f-a6c8-53d12fe393dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8146,  1.2626,  0.2340,  ...,  2.3207, -0.8111,  1.7560],\n",
       "        [ 0.1718,  0.3876, -0.2240,  ...,  0.8188, -0.9657,  0.4134],\n",
       "        [-0.2138,  0.2813, -0.4140,  ...,  0.6651, -0.6380,  0.1997],\n",
       "        ...,\n",
       "        [-0.2061,  0.0312, -0.1428,  ...,  0.0803, -0.0658, -0.1427],\n",
       "        [-0.2038,  0.0046, -0.1370,  ...,  0.0886, -0.0308, -0.1655],\n",
       "        [-0.2196, -0.0142, -0.1603,  ...,  0.0449, -0.0363, -0.1235]])"
      ]
     },
     "execution_count": 722,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_mean[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8acee9-1d83-456c-a1b4-60b42b88bd43",
   "metadata": {},
   "source": [
    "Now we can see how to compute `x_mean` using more efficient means, namely using a lower triangular matrix which can be created using the `tril` method from PyTorch. The trick is to use matrix multiplication instead of `for` loops to replace `x[b,:t+1]` with a zero mask for the time steps in the future which we do not want to use in our `mean` calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8f6554-08f9-4d25-bf3b-b144011a50ca",
   "metadata": {},
   "source": [
    "### Weighted Average: Approach 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "id": "2701d901-0a44-4613-8a30-4544ca36f452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "b=tensor([[2., 8.],\n",
      "        [8., 9.],\n",
      "        [2., 9.]])\n",
      "c=tensor([[12., 26.],\n",
      "        [12., 26.],\n",
      "        [12., 26.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(3,3)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print(f'{a=}')\n",
    "print(f'{b=}')\n",
    "print(f'{c=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9bab80-3f4b-4d27-ba7b-fc72c8d0cad7",
   "metadata": {},
   "source": [
    "To iterate through the time index `t` masking everything in the future from `t+1` onwards we can use a [lower triangular matrix](https://en.wikipedia.org/wiki/Triangular_matrix) using the `torch.tril` function. `tril` is short for lower triangular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "id": "a29c5b13-eb9b-4aa2-955c-67bd8b34b627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 724,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "id": "89cd9e92-67b5-4a8e-91b6-dfb68fa43ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "b=tensor([[6., 3.],\n",
      "        [6., 2.],\n",
      "        [5., 7.]])\n",
      "c=tensor([[ 6.,  3.],\n",
      "        [12.,  5.],\n",
      "        [17., 12.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tril(torch.ones(3,3))\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print(f'{a=}')\n",
    "print(f'{b=}')\n",
    "print(f'{c=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b56a02f-1553-47cc-894b-01f366360b4d",
   "metadata": {},
   "source": [
    "We can take the mean over each previous time step by taking an (equally) weighted average using the lower triangular matrix as the weights instead of using $1.0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "id": "5b7a1808-dc6f-484f-8a59-de5d06528fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "b=tensor([[7., 4.],\n",
      "        [9., 8.],\n",
      "        [9., 4.]])\n",
      "c=tensor([[7.0000, 4.0000],\n",
      "        [8.0000, 6.0000],\n",
      "        [8.3333, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tril(torch.ones(3,3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print(f'{a=}')\n",
    "print(f'{b=}')\n",
    "print(f'{c=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "id": "0010bb19-8aef-4df2-9b83-5b90a6fc6296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0256, 0.0256, 0.0256,  ..., 0.0256, 0.0000, 0.0000],\n",
      "        [0.0250, 0.0250, 0.0250,  ..., 0.0250, 0.0250, 0.0000],\n",
      "        [0.0244, 0.0244, 0.0244,  ..., 0.0244, 0.0244, 0.0244]]) torch.Size([41, 41])\n"
     ]
    }
   ],
   "source": [
    "wavg = torch.tril(torch.ones(block_size, block_size))\n",
    "wavg = wavg / wavg.sum(1, keepdim=True)\n",
    "print(wavg, wavg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "id": "d72fe87f-6d8c-4f15-82cc-ca1968ea57d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.8146,  1.2626,  0.2340,  ...,  2.3207, -0.8111,  1.7560],\n",
      "         [-0.4711, -0.4875, -0.6819,  ..., -0.6831, -1.1202, -0.9291],\n",
      "         [-0.9849,  0.0688, -0.7941,  ...,  0.3577,  0.0175, -0.2276],\n",
      "         ...,\n",
      "         [-0.2601, -1.0996, -1.2506,  ...,  1.4952,  0.3203,  0.8370],\n",
      "         [-0.1123, -1.0322,  0.0894,  ...,  0.4123,  1.3333, -1.0523],\n",
      "         [-0.8514, -0.7698, -1.0955,  ..., -1.7034, -0.2582,  1.5557]],\n",
      "\n",
      "        [[-0.0858,  0.5179, -0.5062,  ..., -2.2934,  3.0562, -0.4245],\n",
      "         [-0.2002, -1.8072,  1.2379,  ..., -0.2050,  0.4118,  0.1319],\n",
      "         [-0.9770,  0.3938,  1.4091,  ...,  0.7232,  1.1109,  1.2289],\n",
      "         ...,\n",
      "         [ 0.2421,  0.1810,  0.4489,  ..., -1.9890,  0.3847,  0.0953],\n",
      "         [-0.5978, -0.4507,  0.5413,  ..., -0.6299,  0.4070,  1.6370],\n",
      "         [-0.3983, -0.6930,  0.5295,  ...,  0.8024,  2.5352, -0.2173]],\n",
      "\n",
      "        [[ 0.0225, -0.3856,  0.0775,  ..., -2.4920,  0.1051,  0.1223],\n",
      "         [ 0.5910,  1.7121, -1.5130,  ..., -0.2807,  0.7349,  0.5217],\n",
      "         [-2.2575, -0.5587,  0.0814,  ...,  0.6159,  0.5453,  0.2271],\n",
      "         ...,\n",
      "         [-0.1094, -1.4182,  1.1472,  ...,  0.7210, -0.2456,  1.8750],\n",
      "         [ 1.2429,  0.0245,  1.0156,  ..., -1.0424, -0.5231, -0.3226],\n",
      "         [-0.3871, -0.6139,  0.7906,  ..., -0.3409,  0.3187, -0.6310]],\n",
      "\n",
      "        [[ 0.1526, -0.1731, -1.7404,  ...,  0.0425, -0.3158,  0.1178],\n",
      "         [ 0.8205,  0.1877, -0.3202,  ..., -0.2988,  0.2799,  0.3879],\n",
      "         [-1.4908,  0.5461, -1.0012,  ...,  0.3569, -0.6742,  1.7321],\n",
      "         ...,\n",
      "         [ 1.0144,  0.8061, -1.0841,  ..., -0.8719,  0.2480, -1.4422],\n",
      "         [-0.4512, -1.4746, -0.4036,  ..., -0.3333, -0.0567, -0.5041],\n",
      "         [ 0.4404,  0.6765, -0.7270,  ...,  1.4623, -0.9601, -0.4501]]]) torch.Size([4, 41, 312])\n"
     ]
    }
   ],
   "source": [
    "print(x, x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6875ff-e0fc-4169-adec-bc0e57e558fd",
   "metadata": {},
   "source": [
    "Take advantage of Pytorch broadcasting to create a batch dimension implicitly for `wavg` so that it can be multiplied with `x`.\n",
    "\n",
    "`wavg` of shape (T, T) becomes (B, T, T) which is multiplied by `x` of shape (B, T, C) to produce a (B, T, C) tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "id": "2cad18ff-1064-4f11-9641-e0907a421fb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 41, 312])"
      ]
     },
     "execution_count": 730,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_mean2 = wavg @ x\n",
    "x_mean2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "id": "50a6568d-ad5a-411c-b24d-355317ab9a3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.8146,  1.2626,  0.2340,  ...,  2.3207, -0.8111,  1.7560],\n",
       "         [ 0.1718,  0.3876, -0.2240,  ...,  0.8188, -0.9657,  0.4134],\n",
       "         [-0.2138,  0.2813, -0.4140,  ...,  0.6651, -0.6380,  0.1997],\n",
       "         ...,\n",
       "         [-0.2061,  0.0312, -0.1428,  ...,  0.0803, -0.0658, -0.1427],\n",
       "         [-0.2038,  0.0046, -0.1370,  ...,  0.0886, -0.0308, -0.1655],\n",
       "         [-0.2196, -0.0142, -0.1603,  ...,  0.0449, -0.0363, -0.1235]]),\n",
       " tensor([[ 0.8146,  1.2626,  0.2340,  ...,  2.3207, -0.8111,  1.7560],\n",
       "         [ 0.1718,  0.3876, -0.2240,  ...,  0.8188, -0.9657,  0.4134],\n",
       "         [-0.2138,  0.2813, -0.4140,  ...,  0.6651, -0.6380,  0.1997],\n",
       "         ...,\n",
       "         [-0.2061,  0.0312, -0.1428,  ...,  0.0803, -0.0658, -0.1427],\n",
       "         [-0.2038,  0.0046, -0.1370,  ...,  0.0886, -0.0308, -0.1655],\n",
       "         [-0.2196, -0.0142, -0.1603,  ...,  0.0449, -0.0363, -0.1235]]))"
      ]
     },
     "execution_count": 731,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_mean[0], x_mean2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "id": "728e187a-4a98-4f9f-9ae1-db0eac327246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 41, 312]) torch.Size([4, 41, 312])\n"
     ]
    }
   ],
   "source": [
    "print(x_mean.shape, x_mean2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855fde6c-2202-4ae3-b33b-d49672bbd193",
   "metadata": {},
   "source": [
    "Check if the compute efficient `x_mean2` is the same as the `for` loop based `x_mean` (from above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "id": "a5fbdf52-d00a-466d-9551-ee9a49cfb0aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 738,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(x_mean, x_mean2, rtol=1e-02)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe805a9-582b-4a21-b542-0981bd984529",
   "metadata": {},
   "source": [
    "### Weighted Average: Approach 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "id": "5ea05873-5e7d-4c22-bd9b-612c0e65af16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "        [0., 0., -inf,  ..., -inf, -inf, -inf],\n",
       "        [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., -inf, -inf],\n",
       "        [0., 0., 0.,  ..., 0., 0., -inf],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 748,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(block_size, block_size))\n",
    "wavg = torch.zeros((block_size, block_size))\n",
    "wavg = wavg.masked_fill(tril == 0, float('-inf'))\n",
    "wavg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "id": "f87911a2-6b09-48bb-9386-9bf94258f7f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        ...,\n",
       "        [0.0256, 0.0256, 0.0256,  ..., 0.0256, 0.0000, 0.0000],\n",
       "        [0.0250, 0.0250, 0.0250,  ..., 0.0250, 0.0250, 0.0000],\n",
       "        [0.0244, 0.0244, 0.0244,  ..., 0.0244, 0.0244, 0.0244]])"
      ]
     },
     "execution_count": 750,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wavg = F.softmax(wavg, dim=-1)\n",
    "wavg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "id": "16f53415-1c32-4781-86de-bbd855378510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8146,  1.2626,  0.2340,  ...,  2.3207, -0.8111,  1.7560],\n",
       "        [ 0.1718,  0.3876, -0.2240,  ...,  0.8188, -0.9657,  0.4134],\n",
       "        [-0.2138,  0.2813, -0.4140,  ...,  0.6651, -0.6380,  0.1997],\n",
       "        ...,\n",
       "        [-0.2061,  0.0312, -0.1428,  ...,  0.0803, -0.0658, -0.1427],\n",
       "        [-0.2038,  0.0046, -0.1370,  ...,  0.0886, -0.0308, -0.1655],\n",
       "        [-0.2196, -0.0142, -0.1603,  ...,  0.0449, -0.0363, -0.1235]])"
      ]
     },
     "execution_count": 751,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_mean3 = wavg @ x\n",
    "x_mean3[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "id": "62add4fa-5c67-44eb-85ff-587adbb7a367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 752,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(x_mean, x_mean3, rtol=1e-02)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

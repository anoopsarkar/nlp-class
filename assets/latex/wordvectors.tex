\input{beamerpreamble.tex}
\begin{document}
\input{titlepage.tex}

\lecture{Word Vectors}{}

\section{One-hot vectors}
\frame{\tableofcontents[currentsection]}

\begin{frame}
\frametitle{One-hot vectors}
\begin{itemize}[<+->]
\item Let $|V|$ be the size of the vocabulary
\item Assign each word to a unique index from $1 \ldots |V|$
\item e.g. {\em aarvark} is $1$, {\em a} is $2$, etc.
\item Represent each word as as a $\mathbb{R}^{|V|\times 1}$
\item The vector has one at index $i$ and all other values are $0$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{One-hot vectors}
\framesubtitle{Figure from \cite{cs224n}}
\begin{center}
\includegraphics[scale=.45]{figures/wordvectors/onehot}	
\end{center}
\end{frame}

\begin{frame}
\frametitle{One-hot vectors}
\begin{itemize}[<+->]
\item Problems with similarity over one-hot vectors 
\item Consider similarity between words as dot product between their word vectors:
\[ w_{\textrm{cat}} \cdot w_{\textrm{dog}} = 
w_{\textrm{joker}} \cdot w_{\textrm{dog}} = 
0 \]
\item Idea: reduce the size of the large sparse one-hot vector
\item Embed large sparse vector into a dense subspace.
\end{itemize}
\end{frame}

\section{Singular Value Decomposition}
\frame{\tableofcontents[currentsection]}

\begin{frame}
\frametitle{Window based co-occurrence matrix}
\begin{itemize}[<+->]
\item Assume a window around each word (window size 2, 5, $\ldots$)
\item Collect co-occurrence counts for each pair of words in the vocabulary.
\item Create a matrix $X$ where each element $X_{i,j} = c(w_i, w_j)$
\item $c(w_i, w_j)$ is the number of times we observe word $w_i$ and $w_j$ together
\item $X$ is going to be very sparse (lots of zeroes)
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Window based co-occurrence matrix}
\begin{center}
\includegraphics[scale=.45]{figures/wordvectors/docs}	
\end{center}
\end{frame}

\begin{frame}
\frametitle{Window based co-occurrence matrix}
\begin{center}
\includegraphics[scale=.4]{figures/wordvectors/wcm}	
\end{center}
\end{frame}

\begin{frame}
\frametitle{Singular Value Decomposition}
\begin{itemize}[<+->]
\item Collect $X = |V| \times |V|$ word co-occurrence matrix.
\item Apply SVD on $X$ to get $X = U S V^T$
\pause
\begin{block}{Transpose}
Transpose of $V$ is $V^T$ which switches the row and column of $V$
\end{block}
\item Select first $k$ columns of $U$ to get $k$-dimensional vectors
\item The matrix $S$ is a diagonal matrix with entries $\sigma_1, \ldots, \sigma_i, \ldots, \sigma_{|V|}$
\end{itemize}
\pause 
\begin{alertblock}{Variance}
The amount of variance captured by the first $k$ dimensions is given by
\[ \frac{\sum_{i=1}^k \sigma_i}{\sum_{i=1}^{|V|} \sigma_i} \]\end{alertblock}
\end{frame}

\begin{frame}
\frametitle{Dimensionality reduction with SVD}
\framesubtitle{Figure from \cite{cs224n}}
\includegraphics[scale=.38]{figures/wordvectors/svdapply}	
\end{frame}

\begin{frame}
\frametitle{Dimensionality reduction with SVD}
\framesubtitle{Figure from \cite{cs224n}}
\includegraphics[scale=.38]{figures/wordvectors/reducedim}	
\end{frame}

\begin{frame}
\frametitle{Why SVD is not the ideal solution}
\begin{itemize}[<+->]
	\item Computational complexity is high ${\cal O}(|V|^3)$
	\item Cannot be trained as part of a larger model. 
	\item It is not a component that can be part of a larger neural network
	\item Cannot be trained discriminatively for a particular task
\end{itemize}
\end{frame}

\section{Word2Vec}
\frame{\tableofcontents[currentsection]}

\begin{frame}
\frametitle{Word2Vec}
\begin{itemize}[<+->]
	\item Word2Vec is a family of model + learning algorithm
	\item The goal is to learn dense word vectors
\end{itemize}
\pause
\begin{alertblock}{Continuous bag of words}
	\begin{itemize}[<+->]
		\item Takes the average of the context; predicts the target word
		\item Trained with gradient descent on cross entropy loss for word prediction 
	\end{itemize}
\end{alertblock}
\pause
\begin{alertblock}{Skip-gram}
	\begin{itemize}[<+->]
		\item Considers each context word independently and constructs (target-word, context-word) pairs
		\item Trained using negative sampling and loss on predicting good vs.\ bad pairs
	\end{itemize}
\end{alertblock}
\end{frame}

\begin{frame}
\frametitle{Word2Vec: Continuous Bag of Words}
\begin{alertblock}{CBOW}
\begin{tabbing}
the \= general \= commanded \= the \= troops\kill \\
the \> general \> \rlap{\underline{\hphantom{commanded}}} \> the \> troops
\end{tabbing}
Predicting a center word from the surrounding words \\
(also window-based)
\end{alertblock}
\pause 
\begin{block}{For each word we want to learn two vectors:}
\begin{itemize}
	\item $v_i \in \mathbb{R}^k$ (input vector) when the word $w_i$ is in the context
	\item $u_i \in \mathbb{R}^k$ (output vector) when the word $u_i$ is in the center 
\end{itemize}
\end{block}
\end{frame}

\begin{frame}
\frametitle{Word2Vec: Continuous Bag of Words}
\begin{block}{Algorithm}
\begin{tabbing}
the \= general \= commanded \= the \= troops\kill \\
the \> general \> \rlap{\underline{\hphantom{commanded}}} \> the \> troops \\
$v_{\textrm{the}}$ \> $v_{\textrm{general}}$ \> \> $v_{\textrm{the}}$ \> $v_{\textrm{troops}}$ 
\end{tabbing}
\begin{itemize}[<+->]
	\item Average the context vectors:
	\[ \hat{v} = \frac{v_{\textrm{the}} + v_{\textrm{general}} + v_{\textrm{the}} + v_{\textrm{troops}}}{4} \]
	\item For each word $i \in V$ we have a word vector $u_i \in \mathbb{R}^k$
	\item Compute the dot product $z_i = u_i \cdot \hat{v}$
	\item Convert $z_i \in \mathbb{R}$ into a probability:
	\[ \hat{y}_i = \frac{exp(z_i)}{\sum_{k=1}^{|V|} exp(z_k)} \]
	\item If the correct center word is $w_i$ then the max should be $\hat{y}_i$.
\end{itemize}	
\end{block}
\end{frame}

\begin{frame}
\frametitle{Word2Vec: Continuous Bag of Words}
\begin{tabbing}
the \= general \= commanded \= the \= troops\kill \\
the \> general \> \rlap{\underline{\hphantom{commanded}}} \> the \> troops \\
$v_{\textrm{the}}$ \> $v_{\textrm{general}}$ \> \> $v_{\textrm{the}}$ \> $v_{\textrm{troops}}$ 
\end{tabbing}
\begin{itemize}[<+->]
	\item Average the context vectors to get $\hat{v}$
	\item Let matrix $U = [ u_1, \ldots, u_{|V|} ] \in \mathbb{R}^{|V| \times k}$ with word vectors $u_i \in \mathbb{R}^k$
	\item Compute the matrix product $z = U \cdot \hat{v}$ where $z = [ z_1, \ldots, z_{|V|} ] \in \mathbb{R}^{|V|}$ and each $z_i \in \mathbb{R}$
	\item Compute vector $\hat{y} \in \mathbb{R}^{|V|}$. Each element $\hat{y}_i = \frac{exp(z_i)}{\sum_{k=1}^{|V|} exp(z_k)} $
	\item We write this as $\hat{y} = \textrm{softmax}(z)$
	\item If the correct center word is $w_i$ then the ideal output $y$ is a one-hot vector with index $i$ as 1 and all other elements are 0.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Word2Vec: Continuous Bag of Words}
\begin{block}{Learning}
\begin{itemize}[<+->]
	\item Goal: learn $k$-dimensional word vectors $u_i, v_i$ for each $i = 1, \ldots |V|$
	\item For each training example the correct center word $w_j$ is represented as a one-hot vector $y$ where $y_j = 1$.
	\item $\hat{y} = \textrm{softmax}(U \cdot \hat{v})$ where $\hat{v}$ is the average of the context words
	\item Loss function is the cross entropy:
	\[ H(\hat{y}, y) = - \log(\hat{y}_j) \textrm{ for $j$ where $y_j = 1$} \]
	\item If $c$ is the index of the correct word, consider case where prediction $\hat{y}_c = 0.99$ then the loss or penalty is low $H(\hat{y}, y) = - 1 \cdot \log(0.99) = 0.01$
	\item If the prediction was bad $\hat{y}_c = 0.01$ then the loss is high $H(\hat{y}, y) = - 1 \cdot \log(0.01) = 4.6$
\end{itemize}	
\end{block}
\end{frame}

\begin{frame}
\frametitle{CBOW Loss Function}
\framesubtitle{Figure from \cite{melamud16}}
\includegraphics[scale=.45]{figures/wordvectors/cbowloss}	
\end{frame}

\begin{frame}
\frametitle{Gradient descent}
\begin{block}{Objective function}
\begin{eqnarray*}
\lefteqn{\textrm{Minimize } J } \\
& = & - \log P(u_c \mid \hat{v}) \\
& = & - u_c \cdot \hat{v} + \log \sum_{j=1}^{|V|} exp(u_j \cdot \hat{v})
\end{eqnarray*}
\end{block}
\end{frame}


\begin{frame}
\frametitle{Gradient descent}
\begin{itemize}[<+->]
\item Initialize $u^{(0)}$ and $v^{(0)}$ 
\item $J(u,v) = - u_c \cdot \hat{v} + \log \sum_{j=1}^{|V|} exp(u_j \cdot \hat{v})$
\item $t \leftarrow 0$
\item Iterate to minimize loss $H(\hat{y}, y)$ on each training example:
\begin{itemize}[<+->]
\item Pick a training example at random
\item Calculate: 
\begin{eqnarray*}
	\hat{y} &=& \textrm{softmax}(U \cdot \hat{v}) \\
	\Delta_u &=& \left. \frac{d J(u,v)}{d u}  \right|_{u,v = u^{(t)}, v^{(t)}} \\
	\Delta_v &=& \left. \frac{d J(u,v)}{d v}  \right|_{u,v = u^{(t)}, v^{(t)}}
\end{eqnarray*}
\item Using a learning rate $\gamma$ find new parameter values:
\begin{eqnarray*}
	\textbf{u}^{(t+1)} &\leftarrow& \textbf{u}^{(t)} - \gamma \Delta_u \\
	\textbf{v}^{(t+1)} &\leftarrow& \textbf{v}^{(t)} - \gamma \Delta_v
\end{eqnarray*}
\end{itemize}
\end{itemize}
\end{frame}

\section{GloVe}
\frame{\tableofcontents[currentsection]}

\begin{frame}
\frametitle{GloVe}
\begin{alertblock}{Co-occurrence matrix}
Let $X$ denote the word-word co-occurrence matrix.\\
$X_{ij}$ is number of times word $j$ occurs in the context of word $i$.\\
Let $X_i = \sum_k X_{ik}$ \\
And $P_{ij} = P(w_j \mid w_i) = \frac{X_{ij}}{X_i}$	
\end{alertblock}
\pause
\begin{alertblock}{GloVe objective}
Probability that word $j$ occurs in context of word $i$:
\[ Q_{ij} = \frac{exp(u_i \cdot v_j)}{\sum_{w=1}^{|V|} exp(u_w \cdot v_i) } \]
Compute global cross-entropy loss:
\[ J = - \sum_{i=1}^{|V|} \sum_{j=1}^{|V|} X_{ij} \log Q_{ij} \]
\end{alertblock}
\end{frame}

\begin{frame}
\frametitle{GloVe}
\begin{alertblock}{Cross Entropy Loss}
\[ J = - \sum_{i=1}^{|V|} \sum_{j=1}^{|V|} \underbrace{X_{ij}}_{X_i P_{ij}} \log Q_{ij} \]
 \[ \textrm{$X_{i,j} = X_i P_{ij}$ because: } P_{ij} = \frac{X_{ij}}{\sum_{k} X_{ik}} = \frac{X_{ij}}{X_i} \]
\[ J = - \sum_i X_i \underbrace{\sum_j P_{ij} \log Q_{ij}}_{H(P_i, Q_i)} \]
where $H$ is the cross entropy of $Q_{ij}$ which uses the parameters $u, v$ wrt the observed frequencies $P_{ij}$.
\end{alertblock}
\end{frame}


\begin{frame}
\frametitle{GloVe}
\begin{alertblock}{Simplify objective function}
 In the objective $- \sum_{ij} X_i \cdot P_{ij} \log Q_{ij}$ the distribution $Q_{ij}$ requires an expensive normalization over the entire vocabulary.\\
Simplify $J$ to $\hat{J}$ using the squared error of the logs of $\hat{P}$ and $\hat{Q}$ without normalization:
\[ \hat{J} = - \sum_{i,j=1}^{|V|} \underbrace{X_i}_{\textrm{replace with function $f(X_{ij})$}} \left( \log \underbrace{\hat{Q}_{ij}}_{exp(u_i \cdot v_j)} - \log \underbrace{\hat{P}_{ij}}_{X_{ij}} \right)^2 \]
\[ \hat{J} = - \sum_{ij} f(X_{ij})  ( u_i \cdot v_j - \log X_{ij} )^2 \] 
The GloVe model efficiently leverages global statistical information by training only on the nonzero elements in a word-word co-occurrence matrix.
\end{alertblock}
\end{frame}

\begin{frame}
\setbeamertemplate{bibliography item}[text]
\begin{thebibliography}{10}

\bibitem{cs224n}
\alert{Christopher Manning, Richard Socher, Francois Chaubard, Michael Fang, Guillaume Genthial, Rohit Mundra.}
\newblock {Natural Language Processing with Deep Learning: Word Vectors I: Introduction, SVD and Word2Vec}
\newblock Winter 2019.


\bibitem{melamud16}
\alert{O. Melamud and J. Goldberger and I. Dagan}
\newblock context2vec: Learning Generic Context Embedding with Bidirectional LSTM.
\newblock CoNLL 2016.
\end{thebibliography}
\end{frame}

\input{acknowledgements.tex}

\end{document}


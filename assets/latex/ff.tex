\input{beamerpreamble.tex}
\begin{document}
\input{titlepage.tex}

\lecture{Feedforward neural networks}

\section{Log-linear models versus Neural networks}
\frame{\tableofcontents[currentsection]}

\begin{frame}{Log linear model}
\begin{itemize}[<+->]
\item Let there be $m$ features, $f_k(\textbf{x}, y)$ for $k = 1, \ldots, m$
\item Define a parameter vector $\textbf{v} \in \mathbb{R}^m$
\item A log-linear model for classification into labels $y \in {\cal Y}$: 
\[ \Pr(y \mid \textbf{x}; \textbf{v}) = \frac{exp\left(\textbf{v} \cdot \textbf{f}(\textbf{x}, y))\right)}{\sum_{y' \in {\cal Y}} exp\left(\textbf{v} \cdot \textbf{f}(\textbf{x}, y'))\right)} \]
\end{itemize}
\pause
\begin{block}{Advantages}
The feature representation $\textbf{f}(\textbf{x}, y)$ can represent any aspect of the input that is useful for classification.
\end{block}
\pause
\begin{block}{Disadvantages}
The feature representation $\textbf{f}(\textbf{x}, y)$ has to be designed by hand which is time-consuming and error-prone.
\end{block}
\end{frame}

\begin{frame}{Log linear model}
\framesubtitle{Figure from \cite{Neubig2018}}
\begin{block}{Disadvantages: number of combined features can explode}
\includegraphics[scale=0.5]{figures/ff/combinedfeats.png}
\end{block}
\end{frame}

\begin{frame}{Neural Networks}
\begin{block}{Advantages}
\begin{itemize}[<+->]
\item Neural networks replace hand-engineered features with \textbf{representation learning}
\item Empirical results across many different domains show that learned representations give significant improvements in accuracy
\item Neural networks allow end to end training for complex NLP tasks and do not have the limitations of multiple chained pipeline models 
\end{itemize}
\end{block}
\pause
\begin{block}{Disadvantages}
For many tasks linear models are much faster to train compared to neural network models
\end{block}
\end{frame}

\begin{frame}{Alternative Form of Log linear model}
\begin{block}{Log-linear model:}
\[ \Pr(y \mid \textbf{x}; \textbf{v}) = \frac{exp\left(\textbf{v} \cdot \textbf{f}(\textbf{x}, y))\right)}{\sum_{y' \in {\cal Y}} exp\left(\textbf{v} \cdot \textbf{f}(\textbf{x}, y'))\right)} \]
\end{block}

\begin{block}{Alternative form using functions:}
\[ \Pr(y \mid x; v) = \frac{exp\left(v(y) \cdot f(x) + \gamma_y \right)}{\sum_{y' \in {\cal Y}} exp\left(v(y') \cdot f(x) + \gamma_{y'})\right)} \]
\end{block}

\begin{itemize}[<+->]
\item Feature vector $f(x)$ maps input $x$ to $\mathbb{R}^d$
\item Parameters $v(y) \in \mathbb{R}^d$ and $\gamma_y \in \mathbb{R}$ for each $y \in {\cal Y}$
\item We assume $v(y) \cdot f(x)$ is a dot product. Using matrix multiplication it would be $v(y) \cdot f(x)^T$
\item Let $v = \{ (v(y), \gamma_y) : y \in {\cal Y} \} $
\end{itemize}
\end{frame}

\section{Feedforward neural networks}
\frame{\tableofcontents[currentsection]}

\begin{frame}{Representation Learning: Feedforward Neural Network}
\begin{block}{Replace hand-engineered features $f$ with learned features $\phi$:}
\[ \Pr(y \mid x; \theta, v) = \frac{exp\left(v(y) \cdot \phi(x;\theta) + \gamma_y \right)}{\sum_{y' \in {\cal Y}} exp\left(v(y') \cdot \phi(x;\theta) + \gamma_{y'})\right)} \]
\end{block}

\begin{itemize}[<+->]
\item Replace $f(x)$ with $\phi(x;\theta) \in \mathbb{R}^d$ where $\theta$ are new parameters
\item Parameters $\theta$ are learned from training data
\item Using $\theta$ the model $\phi$ maps input $x$ to $\mathbb{R}^d$: a learned representation from $x$
\item $x \in \mathbb{R}^d$ is a pre-trained vector of size $d$
\item We will use feedforward neural networks to define $\phi(x;\theta)$
\item $\phi(x;\theta)$ will be a \textbf{non-linear} mapping to $\mathbb{R}^d$ 
\item $\phi$ replaces $f$ which was a \textbf{linear} model
\end{itemize}
\end{frame}

\begin{frame}{A Single Neuron aka Perceptron}
\begin{block}{A single neuron maps input $x \in \mathbb{R}^d$ to output $h$:}
\[ h = g(w \cdot x + b) \]
\end{block}
\begin{itemize}[<+->]
\item Weight vector $w \in \mathbb{R}^d$, a bias $b \in \mathbb{R}$ are the parameters of the model learned from training data
\item \textit{Transfer function} (also called \textit{activation function}) 
\[ g : \mathbb{R} \rightarrow \mathbb{R} \]
\item It is important that $g$ is a \textbf{non-linear} transfer function
\item Linear $g(z) = \alpha \cdot z + \beta$ for constants $\alpha, \beta$ \pause (linear perceptron)
\end{itemize}
\end{frame}

\begin{frame}{Activation Functions and their Gradients}
\framesubtitle{from \cite{Goldberg2017}, Fig.\ 4.3}
\begin{block}{}
\centering
\includegraphics[scale=0.55]{figures/ff/activationfns.png}
\end{block}
\end{frame}

%\begin{frame}{The ReLU Transfer Function $[0,z]$}
%\begin{block}{}
%\centering
%\includegraphics[scale=0.6]{figures/relu.png}
%\end{block}
%\end{frame}

%\begin{frame}{The tanh Transfer Function $[-1,1]$}
%\begin{block}{}
%\centering
%\includegraphics[scale=0.6]{figures/tanh.png}
%\end{block}
%\end{frame}

\begin{frame}{The sigmoid Transfer Function: $\sigma$}
\begin{block}{sigmoid transfer function:}
\[ g(z) = \frac{1}{1 - \textrm{exp}(z)} \]
\end{block}

\pause
\begin{block}{Derivative of sigmoid:}
\[ \frac{d g(z)}{dz} = g(z) (1 - g(z)) \]
\end{block}
\end{frame}
 
\begin{frame}{The tanh Transfer Function}
\begin{block}{tanh transfer function:}
\[ g(z) = \frac {\textrm{exp}(2z) - 1}{\textrm{exp}(2z) + 1} \]
\end{block}

\pause
\begin{block}{Derivative of tanh:}
\[ \frac{d g(z)}{dz} = 1 - g(z)^2 \]
\end{block}

\end{frame}

\begin{frame}{Alternatives to tanh}

\begin{alertblock}{hardtanh:}
\[ g(z) = \left\{ \begin{array}{cc}
 	1 & \textrm{if $z > 1$} \\
 	-1 & \textrm{if $z < -1$} \\
 	z & \textrm{otherwise}
 \end{array}
 \right.
\]
\[ \frac{d g(z)}{dz} = \left\{ \begin{array}{cc}
 	1 & \textrm{if $-1 \leq z \leq 1$} \\
 	0 & \textrm{otherwise}
 \end{array}
 \right. \]
\end{alertblock}

\pause

\begin{alertblock}{softsign:}
\[ g(z) = \frac{z}{1 + |z|} \]
\[ \frac{d g(z)}{dz} = \left\{ \begin{array}{cc}
 	\frac{1}{(1 + z)^2} & \textrm{if $z \geq 0$} \\
 	\frac{-1}{(1 + z)^2} & \textrm{if $z < 0$}
 \end{array}
 \right. \]
\end{alertblock}
\end{frame}

%\begin{frame}{The tanh Transfer Function $[-1,1]$}
%\begin{block}{}
%\centering
%\includegraphics[scale=0.6]{figures/tanh.png}
%\end{block}
%\end{frame}
%\begin{frame}{tanh Gradient}
%\begin{block}{}
%\centering
%\includegraphics[scale=0.6]{figures/tanhgradient.png}
%\end{block}
%\end{frame}

\begin{frame}{The ReLU Transfer Function}
\begin{block}{Rectified Linear Unit (ReLU):}
\[ g(z) = \{ z \textrm{ if } z \geq 0 \textrm{ or } 0 \textrm{ if } z < 0 \} \]
or equivalently $g(z) = \textrm{max}\{0,z\}$
\end{block}

\pause
\begin{block}{Derivative of ReLU:}
\[ \frac{d g(z)}{dz} = \{ 1 \textrm{ if } z > 0 \textrm{ or } 0 \textrm{ if } z < 0 \} \]
non-differentiable or undefined if $z = 0$ \\
(in practice: choose a value for $z = 0$)
\end{block}
\end{frame}

\begin{frame}{The GeLU Transfer Function}
\begin{block}{Gaussian Error Linear Unit (GELU):}
\[ g(z) = \{ \frac{z}{2} (1 + (\sqrt{\frac{2}{\pi}} \times (z + 0.044715 \times z^3))) \} \]
or
\[ g(z) = \{ \frac{z}{2} (1 + \textrm{tanh}(\sqrt{\frac{2}{\pi}} \times (z + 0.044715 \times z^3))) \} \]
Transfer function of choice for Transformer language models.
\end{block}

\pause
\begin{center}
\includegraphics[scale=0.25]{figures/ff/gelu.png}
\end{center}
\end{frame}


\begin{frame}{Desperately Seeking Transfer Functions}
\framesubtitle{from \cite{Ramachandran2017}}
\begin{block}{Enumeration of non-linear functions}
\centering
\includegraphics[scale=0.5]{figures/ff/novelfunctions1.png}
\end{block}
\end{frame}

\begin{frame}{Desperately Seeking Transfer Functions}
\framesubtitle{from \cite{Ramachandran2017}}
\begin{block}{Enumeration of non-linear functions}
\centering
\includegraphics[scale=0.5]{figures/ff/novelfunctions2.png}
\end{block}
\end{frame}

\begin{frame}{The Swish Transfer Function \cite{Ramachandran2017}}

\begin{block}{Enumeration of activation functions:}
Swish was the end result of comparing all the auto-generated activation functions for accuracy on standard datasets.
\end{block}

\begin{block}{Swish uses the sigmoid $\sigma$:}
\[ g(z) = z \cdot \sigma(\beta z) \]
\begin{itemize}[<+->]
	\item If $\beta = 0$ then $g(z) = \frac{z}{2}$ \pause {\small (a linear function; so avoid this)}
	\item If $\beta \rightarrow \infty$ then $g(z) = \textrm{ReLu}$
\end{itemize}
\end{block}

\pause
\begin{block}{Derivative of Swish:}
\[ \frac{d g(z)}{dz} = \beta g(z) + \sigma(\beta z)(1 - \beta g(z)) \]
\end{block}
\end{frame}


\begin{frame}{The Swish Transfer Function \cite{Ramachandran2017}}
    \begin{figure}[ht]
        \begin{minipage}[b]{0.45\linewidth}
            \centering
            \includegraphics[width=\textwidth]{figures/ff/swish.png}
            \caption{Swish transfer function with different values of $\beta$}
        \end{minipage}
        \hspace{0.2cm}
        \begin{minipage}[b]{0.45\linewidth}
            \centering
            \includegraphics[width=\textwidth]{figures/ff/swishderivative.png}
            \caption{First derivative of the Swish transfer function}
        \end{minipage}
    \end{figure}
\end{frame}

\begin{frame}{Derivatives w.r.t.\ parameters}
\begin{block}{Derivatives w.r.t.\ $w$:}
Given 
\[ h = g(w \cdot x + b) \]

derivatives w.r.t.\ $w_1, \ldots, w_j, \ldots w_d$:

\[ \frac{dh}{dw_j} \]
\end{block}

\pause
\begin{block}{Derivatives w.r.t.\ $b$:}
derivatives w.r.t.\ $b$:
\[ \frac{dh}{db} \]
\end{block}
\end{frame}


\begin{frame}{Chain Rule of Differentiation}
\begin{block}{Introduce an intermediate variable $z \in \mathbb{R}$}

\[ z = w \cdot x + b \]
\[ h = g(z) \]

Then by the chain rule to differentiate w.r.t.\ $w$:

\[ \frac{dh}{dw_j} = \textcolor{blue}{\frac{dh}{dz}} \textcolor{red}{\frac{dz}{dw_j}} = \textcolor{blue}{\frac{dg(z)}{dz}} \times \textcolor{red}{x_j}\]

\pause
And similarly for $b$:

\[ \frac{dh}{db} = \textcolor{blue}{\frac{dh}{dz}} \textcolor{red}{\frac{dz}{db}}= \textcolor{blue}{\frac{dg(z)}{dz}} \times \textcolor{red}{1}\]

\end{block}
\end{frame}

\begin{frame}{Single Layer Feedforward model}
\begin{block}{A single layer feedforward model consists of:}
\begin{itemize}[<+->]
\item An integer $d$ specifying the input dimension. Each input to the network is $x \in \mathbb{R}^d$
  \begin{itemize}
  \item Think of it as a $d$ dimensional word embedding
  \end{itemize}
\item An integer $m$ specifying the number of hidden units
\item A parameter matrix $W \in \mathbb{R}^{m \times d}$. The vector $W_k \in \mathbb{R}^d$ for $1 \leq k \leq m$ is the $k$th row of $W$
\item A vector $b \in \mathbb{R}^d$ of bias parameters
\item A transfer function $g : \mathbb{R} \rightarrow \mathbb{R}$\\
$g(z) = \textrm{ReLU}(z)$ or $g(z) = \textrm{tanh}(z)$
\end{itemize}
\end{block}
\end{frame}

\begin{frame}{Single Layer Feedforward model (continued)}
\begin{block}{For $k = 1, \ldots, m$:}
\begin{itemize}[<+->]
\item The input to the $k$th neuron is: 
\( z_k = W_k \cdot x + b_k \)
\item The output from the $k$th neuron is: 
\( h_k = g(z_k) \)
\item Define vector $\phi(x; \theta) \in \mathbb{R}^m$ as:
\( \phi(x; \theta) = h_k \)
\item $\theta = (W, b)$ where $W \in \mathbb{R}^{m \times d}$ and $b \in \mathbb{R}^d$
\item Size of $\theta$ is $m \times (d+1)$ parameters 
\end{itemize}
\end{block}
\pause
\begin{block}{Some intuition}
The neural network employs $m$ hidden units, each with their own parameters $W_k$ and $b_k$, and these neurons are used to construct a \textit{hidden} representation $h \in \mathbb{R}^m$
\end{block}
\end{frame}

\begin{frame}{Matrix Form}
\begin{block}{}
We can replace the operation:

\[ z_k = W_k \cdot x + b \textrm{ for } k = 1, \ldots, m \]

with

\[ z = Wx + b \]

where the dimensions are as follows (vector of size $m$ equals a matrix of size $m \times 1$):

\[ \underbrace{z}_{m \times 1} = \underbrace{\underbrace{W}_{m \times d} \underbrace{x}_{d \times 1}}_{m \times 1} + \underbrace{b}_{m \times 1} \]

\end{block}
\end{frame}

\begin{frame}{Single Layer Feedforward model (matrix form)}
\begin{block}{A single layer feedforward model consists of:}
\begin{itemize}[<+->]
\item An integer $d$ specifying the input dimension. Each input to the network is $x \in \mathbb{R}^d$
\item An integer $m$ specifying the number of hidden units
\item A parameter matrix $W \in \mathbb{R}^{m \times d}$
\item A vector $b \in \mathbb{R}^d$ of bias parameters
\item A transfer function $g : \mathbb{R}^m \rightarrow \mathbb{R}^m$\\
$g(z) = [ \ldots, \textrm{ReLU}(z_i), \ldots ]$ or \\
$g(z) = [ \ldots, \textrm{tanh}(z_i), \ldots ]$ or \\
$g(z) = [ \ldots, \sigma(z_i), \ldots ]$ or \\
for $i = 1, \ldots, m$
\end{itemize}
\end{block}
\end{frame}

\begin{frame}{Single Layer Feedforward model (matrix form, continued)}
\begin{block}{}
\begin{itemize}[<+->]
\item Vector of inputs to the hidden layer $z \in \mathbb{R}^m$: $z = Wx + b$
\item Vector of outputs from hidden layer $h \in \mathbb{R}^m$: $h = g(z)$
\item Define $\phi(x; \theta) = h$ where $\theta = (W, b)$
\item Define $\textrm{softmax}_y = \frac{exp(r_y)}{\sum_{y'} exp(r_{y'})}$ for $r_y = v(y) \cdot h + \gamma_y$
\item Let $V = [ \ldots, v_y, \ldots ]$ for $y \in {\cal Y}$. $v_y \in \mathbb{R}^m$ so $V \in \mathbb{R}^{|{\cal Y}| \times m}$.
\item Let $\Gamma = [ \ldots, \gamma_y, \ldots ]$ for $y \in {\cal Y}$. $\Gamma \in \mathbb{R}^{|{\cal Y}|}$.
\end{itemize}
\end{block}
\pause
\begin{block}{Putting it all together:}
\begin{eqnarray*}
%\Pr(y \mid x; \theta, v) &=& \frac{exp\left(v(y) \cdot \phi(x;\theta) + \gamma_y \right)}{\sum_{y' \in {\cal Y}} exp\left(v(y') \cdot \phi(x;\theta) + \gamma_{y'})\right)} \\
 \underbrace{r}_{\pause\textrm{vector of size $|{\cal Y}|$}} &=& \underbrace{\textrm{softmax}(\underbrace{V \cdot \phi(x;\theta) + \Gamma}_{\pause \text{for each $y \in {\cal Y}$ an $\mathbb{R}$ value}})}_{\pause \text{A vector of size $\mathbb{R}^{{\cal Y}}$ that sums to $1$}}
\end{eqnarray*}
\end{block}
\end{frame}

\begin{frame}{Feedforward neural network}
\begin{block}{}
\centering
\includegraphics[scale=0.4]{figures/ff/ffnet.png}
\end{block}
\end{frame}

\begin{frame}{n-gram Feedforward neural network}
\framesubtitle{from \cite{Bengio2003}}
\begin{block}{}
\centering
\includegraphics[scale=0.4]{figures/ff/ngramfflm.png}
\end{block}
\end{frame}

\section{Stochastic Gradient Descent}
\frame{\tableofcontents[currentsection]}

\begin{frame}{Simple stochastic gradient descent}
\begin{block}{Inputs:}
\begin{itemize}[<+->]
\item Training examples $(x^i, y^i)$ for $i = 1, \ldots, n$
\item A feedforward representation $\phi(x; \theta)$
\item Integer $T$ specifying the number of updates
\item A sequence of learning rates: $\eta^1, \ldots, \eta^T$ where $\eta^t \in [0,1]$
\begin{itemize}[<+->]
\item One should experiment with learning rates: 0.001, 0.01, 0.1, 1
\item Bottou (2012) suggests a learning rate $\eta^t = \frac{\eta^1}{1 + \eta^1 \times \lambda \times t}$ where $\lambda$ is a hyperparameter that can be tuned experimentally
\end{itemize}
\end{itemize}
\end{block}
\pause
\begin{block}{Initialization:}
Set $v = (v(y), \gamma_y)$ for all $y$, and $\theta$ to random values
\end{block}
\end{frame}



\begin{frame}{Gradient descent}
\begin{block}{Algorithm:}
\begin{itemize}[<+->]
\item For $t = 1, \ldots, T$
\begin{itemize}[<+->]
\item Select an integer $i$ uniformly at random from $\{ 1, \ldots, n \}$
\item Define $L(\theta, v) = - \log P(y_i \mid x_i; \theta, v)$
\item For each parameter $\theta_j$ and $v_k(y)$ and $\gamma_y$ (for each label $y$):
\begin{eqnarray*}
\theta_j &=& \theta_j - \eta^t \times \frac{dL(\theta,v)}{d\theta_j} \\
v_k(y) &=& v_k(y) - \eta^t \times \frac{dL(\theta,v)}{d v_k(y)} \\
\gamma(y) &=& \gamma(y) - \eta^t \times \frac{dL(\theta,v)}{d \gamma(y)}
\end{eqnarray*}
\end{itemize}
\item \textbf{Output}: parameters $\theta$, $v = (v(y), \gamma_y)$ for all $y$
\end{itemize}
\end{block}
\end{frame}

\section{Motivating example: XOR}
\frame{\tableofcontents[currentsection]}

\begin{frame}{Motivating example: the XOR problem}
\begin{block}{From \textit{Deep Learning} by Goodfellow, Bengio, Courville}
We will assume a training set where each label is in the set ${\cal Y} = \{ -1, +1 \}$

There are four training examples:
\begin{eqnarray*}
x^1 &=& [0,0], y^1 = -1\\
x^2 &=& [0,1], y^2 = +1 \\
x^3 &=& [1,0], y^3 = +1\\
x^4 &=& [1,1], y^4 = -1
\end{eqnarray*}
\end{block}
\end{frame}

\begin{frame}{Motivating example: the XOR problem}
\begin{block}{}
\includegraphics[scale=0.7]{figures/ff/xorfig.png}
\end{block}
\end{frame}

\begin{frame}{Motivating example: the XOR problem}
\begin{block}{Theorem}
For examples $(x^i, y^i)$ for $i = 1,\ldots,4$ as defined previously for the feedforward neural network:
\[ \Pr(y \mid x; W, b, v) = \frac{exp\left(v(y) \cdot g(Wx + b) + \gamma_y \right)}{\sum_{y' \in {\cal Y}} exp\left(v(y') \cdot g(Wx + b) + \gamma_{y'})\right)} \]
where $x \in \mathbb{R}^2$ ($d=2$) and let $m=2$ so $W \in \mathbb{R}^{2\times2}$ and $b \in \mathbb{R}^2$ and $g$ is a ReLU transfer function.

\pause
Then there are parameter settings $v(-1)$, $v(+1)$, $\gamma_{-1}$, $\gamma_{+1}$, $W, b$ such that 

\[ p(y^i \mid x^i; v) > 0.5 \textrm{   for } i = 1, \ldots, 4 \]
\end{block}
\end{frame}

\begin{frame}{Motivating example: the XOR problem}
\begin{block}{Proof Sketch}
Define $W = \begin{bmatrix}
 1 & 1 \\
 1 & 1 
 \end{bmatrix}$ 
and $b = \begin{bmatrix} 0 \\ -1 \end{bmatrix}$
\pause
Then for each input $x$ calculate values of $z = Wx+b$ and $h=g(z)$:

\begin{eqnarray*}
x = [0,0] &\Rightarrow& z = [0,-1] \Rightarrow h = [0,0] \\
x = [1,0] &\Rightarrow& z = [1,0] \Rightarrow h = [1,0] \\
x = [0,1] &\Rightarrow& z = [1,0] \Rightarrow h = [1,0] \\
x = [1,1] &\Rightarrow& z = [2,1] \Rightarrow h = [2,1] 
\end{eqnarray*}
\end{block}
\end{frame}

\begin{frame}{Motivating example: the XOR problem}
\begin{block}{Proof Sketch (continued)}
\begin{eqnarray*}
p(+1 \mid x; v) &=& \frac{exp(v(+1) \cdot h  + \gamma_{+1})}{exp(v(+1) \cdot h  + \gamma_{+1}) + exp(v(-1) \cdot h + \gamma_{-1})} \\
&=& \frac{1}{1 + exp(-(u \cdot h + \gamma))}
\end{eqnarray*}

\pause
To satisfy $P(y^i \mid x^i; v) > 0.5$ for $i = 1,\ldots,4$
we have to find parameters $u = v(+1) - v(-1)$ and $\gamma = \gamma_{+1} - \gamma_{-1}$
such that:

\begin{eqnarray*}
u \cdot [0,0] + \gamma &<& 0 \\
u \cdot [1,0] + \gamma &>& 0 \\
u \cdot [1,0] + \gamma &>& 0 \\
u \cdot [2,1] + \gamma &<& 0
\end{eqnarray*}

$u = [1, -2]$ and $\gamma = -0.5$ satisfies these constraints.
\end{block}
\end{frame}

\begin{frame}{Solving the XOR problem}
\begin{figure}
   \includegraphics[width=0.475\textwidth]{figures/ff/xorbefore.png}
   \hfill
   \includegraphics[width=0.475\textwidth]{figures/ff/xorafter.png}
\end{figure}
\end{frame}

\section{Computation Graphs}
\frame{\tableofcontents[currentsection]}

\begin{frame}{Complex neural networks}
\begin{block}{Neural network with a loss function}
Consider a neural network trained using a \textbf{squared-error loss}. 
For the correct answer $y^\ast$ the output value $y$ is compared using
the function $(y^\ast - y)^2$.

	\begin{eqnarray*}
		h' &=& W_{xh} x + b_h \\
		h &=& \text{tanh}(h') \\
		y &=& w_{hy} h + b_y \\
		\ell &=& (y^\ast - y)^2
	\end{eqnarray*}
\end{block}

\end{frame}

\begin{frame}[shrink=5]{Derivative wrt loss}
\begin{block}{}
	\begin{eqnarray*}
		h' &=& W_{xh} x + b_h \\
		h &=& \text{tanh}(h') \\
		y &=& w_{hy} h + b_y \\
		\ell &=& (y^\ast - y)^2
	\end{eqnarray*}
	
	We want to compute $\frac{d \ell}{d b_y}$, $\frac{d \ell}{d w_{hy}}$, $\frac{d \ell}{d b_h}$, $\frac{d \ell}{d W_{xh}}$
\end{block}
\pause
\begin{block}{}
	\begin{eqnarray*}
		\frac{d \ell}{d b_y} &=& {\color{red} \frac{d \ell}{d y}} \pause \frac{d y}{d b_y} \pause \\
		\frac{d \ell}{d w_{hy}} &=& {\color{red} \frac{d \ell}{d y}} \pause \frac{d y}{d w_{hy}} \pause \\
		\frac{d \ell}{d b_h} &=& {\color{red} \frac{d \ell}{d y}} \pause {\color{blue} \frac{d y}{d h}} {\color{purple} \frac{d h}{d h'}} \pause \frac{d h'}{d b_h} \pause \\
		\frac{d \ell}{d W_{xh}} &=& {\color{red} \pause \frac{d \ell}{d y}} \pause {\color{blue} \frac{d y}{d h}} \pause {\color{purple} \frac{d h}{d h'}}\frac{d h'}{d W_{xh}} 
	\end{eqnarray*}
\end{block}

\end{frame}

\begin{frame}{Computation graphs and automatic differentiation}
\framesubtitle{Figure from \cite{Neubig2018}}
\includegraphics[scale=0.5]{figures/ff/compgraph.png}
\end{frame}

\begin{frame}{Computation graphs and automatic differentiation}
\begin{itemize}[<+->]
\item Automatic differentiation is a two-step dynamic programming algorithm that operates
over the second graph and performs:
\begin{description}
\item[Forward calculation] which traverses the nodes in the graph in topological order,
calculating the actual result of the computation.
\item[Back propagation] which traverses the nodes in reverse topological order, calculating
the gradients.
\end{description}
\item Many neural network toolkits can perform auto differentiation for very large computation graphs.
\end{itemize}
\end{frame}

\begin{frame}
\setbeamertemplate{bibliography item}[text]
\begin{thebibliography}{10}

\bibitem{Neubig2018}
\alert{Graham Neubig}
\newblock {Neural Networks for NLP}
\newblock 2018.

\bibitem{Goldberg2017}
\alert{Yoav Goldberg}
\newblock {Neural Network Methods for Natural Language Processing}
\newblock 2017.

\bibitem{Ramachandran2017}
\alert{Prajit Ramachandran, Barret Zoph, Quoc V. Le}
\newblock {Searching for Activation Functions}
\newblock 2017.

\bibitem{Glorot2010}
\alert{Xavier Glorot, Yoshua Bengio}
\newblock {Understanding the difficulty of training deep feedforward neural networks}
\newblock 2010.

\bibitem{Bengio2003}
\alert{Yoshua Bengio, R\'ejean Ducharme, Pascal Vincent, Christian Jauvin}
\newblock {A Neural Probabilistic Language Model}
\newblock 2003.

\end{thebibliography}
\end{frame}

\input{acknowledgements.tex}

\end{document}
 
 
